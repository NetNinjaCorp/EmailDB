using System;
using System.Collections.Generic;
using System.Diagnostics;
using System.IO;
using System.Linq;
using System.Threading.Tasks;
using EmailDB.Format.FileManagement;
using EmailDB.Format.Models;
using Force.Crc32;
using Xunit;
using Xunit.Abstractions;

namespace EmailDB.UnitTests;

/// <summary>
/// Tests focused on EmailDB's resilience features:
/// - Corruption detection and recovery
/// - Append-only architecture benefits
/// - Version preservation and rollback capabilities
/// - File integrity under various failure scenarios
/// </summary>
public class EmailDBResilienceTests : IDisposable
{
    private readonly string _testFile;
    private readonly RawBlockManager _blockManager;
    private readonly ITestOutputHelper _output;

    public EmailDBResilienceTests(ITestOutputHelper output)
    {
        _output = output;
        _testFile = Path.GetTempFileName();
        _blockManager = new RawBlockManager(_testFile);
    }

    [Fact]
    public async Task Resilience_Detect_And_Skip_Corrupted_Blocks()
    {
        const int blockCount = 100;
        var random = new Random(1000);
        var validBlockIds = new List<long>();

        _output.WriteLine("=== Corruption Detection and Recovery Test ===");

        // Write valid blocks
        for (int i = 0; i < blockCount; i++)
        {
            var data = new byte[256];
            random.NextBytes(data);
            
            var block = new Block
            {
                Version = 1,
                Type = BlockType.Segment,
                Flags = 0,
                Encoding = PayloadEncoding.RawBytes,
                Timestamp = DateTime.UtcNow.Ticks,
                BlockId = 400000 + i,
                Payload = data
            };

            var result = await _blockManager.WriteBlockAsync(block);
            if (result.IsSuccess)
                validBlockIds.Add(block.BlockId);
        }

        _output.WriteLine($"Wrote {validBlockIds.Count} valid blocks");

        // Simulate corruption by directly modifying the file
        _blockManager.Dispose(); // Close file handle
        
        using (var fs = new FileStream(_testFile, FileMode.Open, FileAccess.ReadWrite))
        {
            // Corrupt a block in the middle
            fs.Seek(fs.Length / 2, SeekOrigin.Begin);
            var corruptData = new byte[100];
            random.NextBytes(corruptData);
            fs.Write(corruptData, 0, corruptData.Length);
            
            _output.WriteLine($"Introduced corruption at position {fs.Position}");
        }

        // Reopen and test recovery
        _blockManager = new RawBlockManager(_testFile);
        
        _output.WriteLine("\nAttempting to read blocks after corruption...");
        var readableCount = 0;
        var corruptedCount = 0;
        
        foreach (var blockId in validBlockIds)
        {
            var result = await _blockManager.ReadBlockAsync(blockId);
            if (result.IsSuccess)
            {
                readableCount++;
            }
            else
            {
                corruptedCount++;
                _output.WriteLine($"Block {blockId} corrupted: {result.Error}");
            }
        }

        _output.WriteLine($"\nRecovery results:");
        _output.WriteLine($"- Readable blocks: {readableCount}/{validBlockIds.Count}");
        _output.WriteLine($"- Corrupted blocks: {corruptedCount}");
        _output.WriteLine($"- Recovery rate: {(readableCount / (double)validBlockIds.Count * 100):F1}%");

        // Verify at least some blocks are still readable
        Assert.True(readableCount > 0, "Should be able to read some blocks despite corruption");
    }

    [Fact]
    public async Task Resilience_Preserve_All_Block_Versions()
    {
        const int blockId = 500000;
        const int versions = 10;
        var random = new Random(2000);
        var versionData = new Dictionary<int, byte[]>();

        _output.WriteLine("=== Version Preservation Test ===");
        _output.WriteLine($"Writing {versions} versions of block {blockId}");

        // Write multiple versions of the same block
        for (int v = 1; v <= versions; v++)
        {
            var data = new byte[128 + v * 50]; // Different size for each version
            random.NextBytes(data);
            versionData[v] = data;
            
            var block = new Block
            {
                Version = (ushort)v,
                Type = BlockType.Segment,
                Flags = (byte)(v % 3), // Vary flags too
                Encoding = PayloadEncoding.RawBytes,
                Timestamp = DateTime.UtcNow.Ticks + v * 1000,
                BlockId = blockId,
                Payload = data
            };

            var result = await _blockManager.WriteBlockAsync(block);
            Assert.True(result.IsSuccess);
            
            _output.WriteLine($"- Version {v}: {data.Length} bytes written");
        }

        // Current read should return latest version
        var latestRead = await _blockManager.ReadBlockAsync(blockId);
        Assert.True(latestRead.IsSuccess);
        Assert.Equal(versions, latestRead.Value.Version);
        Assert.Equal(versionData[versions], latestRead.Value.Payload);
        
        _output.WriteLine($"\nLatest version ({versions}) correctly returned");

        // Verify file contains all versions
        var fileSize = new FileInfo(_testFile).Length;
        var expectedMinSize = versionData.Values.Sum(d => d.Length) + (versions * 61); // Data + overhead
        
        _output.WriteLine($"\nFile size verification:");
        _output.WriteLine($"- Actual file size: {fileSize:N0} bytes");
        _output.WriteLine($"- Minimum expected: {expectedMinSize:N0} bytes");
        Assert.True(fileSize >= expectedMinSize, "File should contain all versions");

        // In a real implementation, we could add a method to read specific versions
        _output.WriteLine("\nAll versions preserved in append-only format");
    }

    [Fact]
    public async Task Resilience_Recover_From_Incomplete_Write()
    {
        const int completeBlocks = 50;
        var random = new Random(3000);

        _output.WriteLine("=== Incomplete Write Recovery Test ===");

        // Write complete blocks
        for (int i = 0; i < completeBlocks; i++)
        {
            var data = new byte[512];
            random.NextBytes(data);
            
            var block = new Block
            {
                Version = 1,
                Type = BlockType.Segment,
                Flags = 0,
                Encoding = PayloadEncoding.RawBytes,
                Timestamp = DateTime.UtcNow.Ticks,
                BlockId = 600000 + i,
                Payload = data
            };

            await _blockManager.WriteBlockAsync(block);
        }

        var validFileSize = new FileInfo(_testFile).Length;
        _output.WriteLine($"Wrote {completeBlocks} complete blocks, file size: {validFileSize:N0} bytes");

        // Simulate incomplete write by appending partial data
        _blockManager.Dispose();
        
        using (var fs = new FileStream(_testFile, FileMode.Append, FileAccess.Write))
        {
            // Write partial header (incomplete block)
            var partialHeader = new byte[20]; // Less than full header
            random.NextBytes(partialHeader);
            fs.Write(partialHeader, 0, partialHeader.Length);
            
            _output.WriteLine($"Appended {partialHeader.Length} bytes of incomplete block");
        }

        // Reopen and verify recovery
        _blockManager = new RawBlockManager(_testFile);
        
        _output.WriteLine("\nVerifying recovery from incomplete write...");
        var readableCount = 0;
        
        for (int i = 0; i < completeBlocks; i++)
        {
            var result = await _blockManager.ReadBlockAsync(600000 + i);
            if (result.IsSuccess)
                readableCount++;
        }

        Assert.Equal(completeBlocks, readableCount);
        _output.WriteLine($"Successfully read all {readableCount} complete blocks");
        _output.WriteLine("Incomplete block at end ignored during read operations");
    }

    [Fact]
    public async Task Resilience_Handle_Power_Loss_Scenario()
    {
        const int blocksBeforeCrash = 100;
        const int blocksAfterRecovery = 50;
        var random = new Random(4000);

        _output.WriteLine("=== Power Loss Simulation Test ===");

        // Phase 1: Normal operations
        _output.WriteLine("Phase 1: Normal operations before 'power loss'");
        
        for (int i = 0; i < blocksBeforeCrash; i++)
        {
            var data = new byte[256];
            random.NextBytes(data);
            
            var block = new Block
            {
                Version = 1,
                Type = BlockType.Segment,
                Flags = 0,
                Encoding = PayloadEncoding.RawBytes,
                Timestamp = DateTime.UtcNow.Ticks,
                BlockId = 700000 + i,
                Payload = data
            };

            await _blockManager.WriteBlockAsync(block);
        }

        var beforeCrashSize = new FileInfo(_testFile).Length;
        _output.WriteLine($"- Wrote {blocksBeforeCrash} blocks");
        _output.WriteLine($"- File size: {beforeCrashSize:N0} bytes");

        // Simulate sudden shutdown
        _output.WriteLine("\n[SIMULATING POWER LOSS]");
        _blockManager.Dispose();

        // Phase 2: Recovery and continued operation
        _output.WriteLine("\nPhase 2: System recovery");
        _blockManager = new RawBlockManager(_testFile);

        // Verify pre-crash data
        var verifiedCount = 0;
        for (int i = 0; i < blocksBeforeCrash; i++)
        {
            var result = await _blockManager.ReadBlockAsync(700000 + i);
            if (result.IsSuccess)
                verifiedCount++;
        }

        Assert.Equal(blocksBeforeCrash, verifiedCount);
        _output.WriteLine($"- Verified {verifiedCount} pre-crash blocks intact");

        // Continue operations
        _output.WriteLine("\nPhase 3: Continuing operations after recovery");
        
        for (int i = 0; i < blocksAfterRecovery; i++)
        {
            var data = new byte[512];
            random.NextBytes(data);
            
            var block = new Block
            {
                Version = 1,
                Type = BlockType.Segment,
                Flags = 0,
                Encoding = PayloadEncoding.RawBytes,
                Timestamp = DateTime.UtcNow.Ticks,
                BlockId = 800000 + i,
                Payload = data
            };

            await _blockManager.WriteBlockAsync(block);
        }

        var finalSize = new FileInfo(_testFile).Length;
        _output.WriteLine($"- Wrote {blocksAfterRecovery} additional blocks");
        _output.WriteLine($"- Final file size: {finalSize:N0} bytes");

        // Verify all data accessible
        var totalVerified = 0;
        
        // Pre-crash blocks
        for (int i = 0; i < blocksBeforeCrash; i++)
        {
            var result = await _blockManager.ReadBlockAsync(700000 + i);
            if (result.IsSuccess) totalVerified++;
        }
        
        // Post-recovery blocks
        for (int i = 0; i < blocksAfterRecovery; i++)
        {
            var result = await _blockManager.ReadBlockAsync(800000 + i);
            if (result.IsSuccess) totalVerified++;
        }

        Assert.Equal(blocksBeforeCrash + blocksAfterRecovery, totalVerified);
        _output.WriteLine($"\nAll {totalVerified} blocks accessible after power loss recovery");
    }

    [Fact]
    public async Task Resilience_Checksum_Validation()
    {
        const int blockCount = 100;
        var random = new Random(5000);

        _output.WriteLine("=== Checksum Validation Test ===");

        // Write blocks with verified checksums
        var blockData = new Dictionary<long, (byte[] data, uint checksum)>();
        
        for (int i = 0; i < blockCount; i++)
        {
            var data = new byte[256];
            random.NextBytes(data);
            var checksum = Crc32Algorithm.Compute(data);
            
            var block = new Block
            {
                Version = 1,
                Type = BlockType.Segment,
                Flags = 0,
                Encoding = PayloadEncoding.RawBytes,
                Timestamp = DateTime.UtcNow.Ticks,
                BlockId = 900000 + i,
                Payload = data
            };

            var result = await _blockManager.WriteBlockAsync(block);
            Assert.True(result.IsSuccess);
            
            blockData[block.BlockId] = (data, checksum);
        }

        _output.WriteLine($"Wrote {blockCount} blocks with checksums");

        // Verify checksums on read
        _output.WriteLine("\nValidating checksums on read...");
        var checksumMatches = 0;
        
        foreach (var kvp in blockData)
        {
            var result = await _blockManager.ReadBlockAsync(kvp.Key);
            Assert.True(result.IsSuccess);
            
            var readChecksum = Crc32Algorithm.Compute(result.Value.Payload);
            if (readChecksum == kvp.Value.checksum)
            {
                checksumMatches++;
            }
            else
            {
                _output.WriteLine($"Checksum mismatch for block {kvp.Key}!");
            }
        }

        Assert.Equal(blockCount, checksumMatches);
        _output.WriteLine($"All {checksumMatches} checksums validated successfully");

        // Test corruption detection via checksum
        _output.WriteLine("\nTesting checksum-based corruption detection...");
        
        // This would require internal access to corrupt specific bytes
        // For now, we verify that the checksum system is in place
        var sampleBlock = await _blockManager.ReadBlockAsync(900000);
        Assert.True(sampleBlock.IsSuccess);
        Assert.NotEqual(0u, sampleBlock.Value.HeaderChecksum);
        Assert.NotEqual(0u, sampleBlock.Value.PayloadChecksum);
        
        _output.WriteLine("Checksum fields properly populated for corruption detection");
    }

    [Fact]
    public async Task Resilience_Large_Scale_Update_Rollback_Capability()
    {
        const int initialBlocks = 1000;
        const int updateBatch = 200;
        var random = new Random(6000);

        _output.WriteLine("=== Large Scale Update & Rollback Capability Test ===");

        // Create initial state
        _output.WriteLine($"Creating initial state with {initialBlocks} blocks...");
        var originalData = new Dictionary<long, byte[]>();
        
        for (int i = 0; i < initialBlocks; i++)
        {
            var data = new byte[256];
            random.NextBytes(data);
            
            var blockId = 1000000 + i;
            originalData[blockId] = data;
            
            var block = new Block
            {
                Version = 1,
                Type = BlockType.Segment,
                Flags = 0,
                Encoding = PayloadEncoding.RawBytes,
                Timestamp = DateTime.UtcNow.AddHours(-1).Ticks, // 1 hour ago
                BlockId = blockId,
                Payload = data
            };

            await _blockManager.WriteBlockAsync(block);
        }

        var stateOneSize = new FileInfo(_testFile).Length;
        _output.WriteLine($"Initial state size: {stateOneSize:N0} bytes");

        // Perform large batch update
        _output.WriteLine($"\nPerforming batch update of {updateBatch} blocks...");
        var updateTimestamp = DateTime.UtcNow.Ticks;
        var updatedBlocks = new HashSet<long>();
        
        for (int i = 0; i < updateBatch; i++)
        {
            var blockId = 1000000 + random.Next(initialBlocks);
            updatedBlocks.Add(blockId);
            
            var newData = new byte[512]; // Larger size
            random.NextBytes(newData);
            
            var block = new Block
            {
                Version = 2,
                Type = BlockType.Segment,
                Flags = 1, // Mark as updated
                Encoding = PayloadEncoding.RawBytes,
                Timestamp = updateTimestamp,
                BlockId = blockId,
                Payload = newData
            };

            await _blockManager.WriteBlockAsync(block);
        }

        var stateTwoSize = new FileInfo(_testFile).Length;
        _output.WriteLine($"After update size: {stateTwoSize:N0} bytes");
        _output.WriteLine($"Size increase: {stateTwoSize - stateOneSize:N0} bytes");

        // Verify current state shows updates
        _output.WriteLine("\nVerifying updated blocks return new versions...");
        var verifiedUpdates = 0;
        
        foreach (var blockId in updatedBlocks.Take(10)) // Sample
        {
            var result = await _blockManager.ReadBlockAsync(blockId);
            Assert.True(result.IsSuccess);
            if (result.Value.Version == 2 && result.Value.Flags == 1)
                verifiedUpdates++;
        }
        
        Assert.Equal(10, verifiedUpdates);
        _output.WriteLine($"Updates verified: Latest versions returned");

        // Demonstrate rollback capability
        _output.WriteLine("\nDemonstrating rollback capability:");
        _output.WriteLine("- Original data still exists in file (append-only)");
        _output.WriteLine("- With timestamp-based filtering, could read pre-update state");
        _output.WriteLine($"- Update timestamp: {updateTimestamp}");
        _output.WriteLine("- A rollback would ignore blocks with timestamp >= update time");
        
        // Calculate space that could be reclaimed
        var potentialReclaim = stateTwoSize - stateOneSize;
        _output.WriteLine($"\nSpace that could be reclaimed by cleanup: {potentialReclaim:N0} bytes");
    }

    public void Dispose()
    {
        _blockManager?.Dispose();
        
        if (File.Exists(_testFile))
        {
            try
            {
                File.Delete(_testFile);
            }
            catch
            {
                // Best effort
            }
        }
    }
}