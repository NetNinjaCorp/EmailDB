using System;
using System.Collections.Generic;
using System.Diagnostics;
using System.IO;
using System.Linq;
using System.Threading.Tasks;
using EmailDB.Format.FileManagement;
using EmailDB.Format.Models;
using EmailDB.Format.Models.BlockTypes;
using Xunit;
using Xunit.Abstractions;

namespace EmailDB.UnitTests;

/// <summary>
/// Stress tests for EmailDB focusing on:
/// - High volume block operations
/// - Performance benchmarks
/// - Recovery and cleanup scenarios
/// - Index rebuild performance
/// </summary>
public class EmailDBStressTests : IDisposable
{
    private readonly string _testFile;
    private readonly RawBlockManager _blockManager;
    private readonly ITestOutputHelper _output;

    public EmailDBStressTests(ITestOutputHelper output)
    {
        _output = output;
        _testFile = Path.GetTempFileName();
        _blockManager = new RawBlockManager(_testFile);
    }

    [Fact]
    public async Task StressTest_Write_Thousands_Of_Blocks()
    {
        const int blockCount = 5000;
        var random = new Random(42);
        var stopwatch = Stopwatch.StartNew();
        var writtenBlocks = new List<long>();

        _output.WriteLine($"=== Starting stress test: Writing {blockCount:N0} blocks ===");

        // Phase 1: Write blocks
        for (int i = 0; i < blockCount; i++)
        {
            var dataSize = 100 + random.Next(1900); // 100 to 2000 bytes
            var data = new byte[dataSize];
            random.NextBytes(data);

            var block = new Block
            {
                Version = 1,
                Type = (BlockType)(i % 5 + 1), // Vary block types
                Flags = 0,
                Encoding = PayloadEncoding.RawBytes,
                Timestamp = DateTime.UtcNow.Ticks,
                BlockId = 10000 + i,
                Payload = data
            };

            var writeResult = await _blockManager.WriteBlockAsync(block);
            Assert.True(writeResult.IsSuccess);
            writtenBlocks.Add(block.BlockId);

            if ((i + 1) % 1000 == 0)
            {
                _output.WriteLine($"Progress: {i + 1:N0} blocks written");
            }
        }

        stopwatch.Stop();
        var writeTime = stopwatch.ElapsedMilliseconds;
        var avgWriteTime = writeTime / (double)blockCount;

        _output.WriteLine($"\nWrite Performance:");
        _output.WriteLine($"- Total blocks written: {blockCount:N0}");
        _output.WriteLine($"- Total write time: {writeTime:N0}ms");
        _output.WriteLine($"- Average write time per block: {avgWriteTime:F3}ms");
        _output.WriteLine($"- Write throughput: {blockCount / (writeTime / 1000.0):F2} blocks/second");

        // Phase 2: Random read test
        stopwatch.Restart();
        const int readCount = 100;
        var readIndexes = new HashSet<int>();
        while (readIndexes.Count < readCount)
        {
            readIndexes.Add(random.Next(blockCount));
        }

        foreach (var index in readIndexes)
        {
            var readResult = await _blockManager.ReadBlockAsync(writtenBlocks[index]);
            Assert.True(readResult.IsSuccess);
        }

        stopwatch.Stop();
        var readTime = stopwatch.ElapsedMilliseconds;
        var avgReadTime = readTime / (double)readCount;

        _output.WriteLine($"\nRandom Read Performance:");
        _output.WriteLine($"- Blocks read: {readCount}");
        _output.WriteLine($"- Total read time: {readTime:N0}ms");
        _output.WriteLine($"- Average read time per block: {avgReadTime:F3}ms");

        // Phase 3: File size analysis
        var fileInfo = new FileInfo(_testFile);
        var avgBlockSize = fileInfo.Length / (double)blockCount;
        
        _output.WriteLine($"\nFile Statistics:");
        _output.WriteLine($"- Final file size: {fileInfo.Length:N0} bytes ({fileInfo.Length / 1024.0 / 1024.0:F2} MB)");
        _output.WriteLine($"- Average bytes per block: {avgBlockSize:F2}");
    }

    [Fact]
    public async Task StressTest_Delete_And_Cleanup_Scenario()
    {
        const int initialBlocks = 1000;
        const int blocksToDelete = 100;
        var random = new Random(123);
        var cacheManager = new CacheManager(_blockManager);
        var metadataManager = new MetadataManager(cacheManager);
        var segmentManager = new SegmentManager(cacheManager, metadataManager);

        _output.WriteLine($"=== Delete and Cleanup Stress Test ===");
        _output.WriteLine($"Creating {initialBlocks:N0} blocks...");

        // Create initial blocks
        var segmentIds = new List<long>();
        for (int i = 0; i < initialBlocks; i++)
        {
            var data = new byte[512];
            random.NextBytes(data);
            
            var segment = await segmentManager.CreateSegmentAsync(data);
            segmentIds.Add(segment.SegmentId);
        }

        var initialFileSize = new FileInfo(_testFile).Length;
        _output.WriteLine($"Initial file size: {initialFileSize:N0} bytes");

        // Mark random blocks for deletion
        var toDelete = segmentIds.OrderBy(x => random.Next()).Take(blocksToDelete).ToList();
        
        _output.WriteLine($"\nMarking {blocksToDelete} blocks as outdated...");
        var stopwatch = Stopwatch.StartNew();
        
        foreach (var segmentId in toDelete)
        {
            await segmentManager.DeleteSegmentAsync(segmentId);
        }
        
        stopwatch.Stop();
        _output.WriteLine($"Deletion marking time: {stopwatch.ElapsedMilliseconds}ms");

        // Verify outdated blocks
        var outdatedCount = 0;
        foreach (var segmentId in toDelete)
        {
            if (await segmentManager.IsSegmentOutdatedAsync(segmentId))
                outdatedCount++;
        }
        
        Assert.Equal(blocksToDelete, outdatedCount);
        _output.WriteLine($"Verified {outdatedCount} blocks marked as outdated");

        // Cleanup operation
        _output.WriteLine("\nPerforming cleanup...");
        stopwatch.Restart();
        
        var cleanupResult = await segmentManager.CleanupOutdatedSegmentsAsync();
        Assert.True(cleanupResult.IsSuccess);
        
        stopwatch.Stop();
        _output.WriteLine($"Cleanup time: {stopwatch.ElapsedMilliseconds}ms");

        // Verify remaining blocks are still accessible
        var remainingBlocks = segmentIds.Except(toDelete).ToList();
        var verifiedCount = 0;
        
        foreach (var segmentId in remainingBlocks.Take(50)) // Verify sample
        {
            var segment = await segmentManager.GetSegmentAsync(segmentId);
            if (segment != null)
                verifiedCount++;
        }
        
        _output.WriteLine($"Verified {verifiedCount}/50 remaining blocks are still accessible");
    }

    [Fact]
    public async Task Performance_Block_Location_Index_Speed()
    {
        const int blockCount = 10000;
        var random = new Random(456);
        var blockIds = new List<long>();

        _output.WriteLine($"=== Block Location Index Performance Test ===");
        
        // Create blocks
        _output.WriteLine($"Creating {blockCount:N0} blocks for index testing...");
        
        for (int i = 0; i < blockCount; i++)
        {
            var data = new byte[256];
            random.NextBytes(data);
            
            var block = new Block
            {
                Version = 1,
                Type = BlockType.Segment,
                Flags = 0,
                Encoding = PayloadEncoding.RawBytes,
                Timestamp = DateTime.UtcNow.Ticks,
                BlockId = 20000 + i,
                Payload = data
            };

            await _blockManager.WriteBlockAsync(block);
            blockIds.Add(block.BlockId);
        }

        // Test 1: Time to locate a specific block
        _output.WriteLine("\nTesting single block lookup performance...");
        var lookupTimes = new List<double>();
        
        for (int i = 0; i < 100; i++)
        {
            var targetId = blockIds[random.Next(blockIds.Count)];
            var stopwatch = Stopwatch.StartNew();
            
            var result = await _blockManager.ReadBlockAsync(targetId);
            
            stopwatch.Stop();
            Assert.True(result.IsSuccess);
            lookupTimes.Add(stopwatch.Elapsed.TotalMilliseconds);
        }

        var avgLookupTime = lookupTimes.Average();
        var minLookupTime = lookupTimes.Min();
        var maxLookupTime = lookupTimes.Max();

        _output.WriteLine($"Block lookup performance (100 random lookups):");
        _output.WriteLine($"- Average: {avgLookupTime:F3}ms");
        _output.WriteLine($"- Min: {minLookupTime:F3}ms");
        _output.WriteLine($"- Max: {maxLookupTime:F3}ms");

        // Test 2: Time to build complete index
        _output.WriteLine("\nTesting index rebuild performance...");
        var rebuildStopwatch = Stopwatch.StartNew();
        
        var allLocations = _blockManager.GetBlockLocations();
        
        rebuildStopwatch.Stop();
        
        Assert.Equal(blockCount, allLocations.Count);
        _output.WriteLine($"Index rebuild results:");
        _output.WriteLine($"- Blocks indexed: {allLocations.Count:N0}");
        _output.WriteLine($"- Rebuild time: {rebuildStopwatch.ElapsedMilliseconds:N0}ms");
        _output.WriteLine($"- Speed: {allLocations.Count / (rebuildStopwatch.ElapsedMilliseconds / 1000.0):F2} blocks/second");
    }

    [Fact]
    public async Task Resilience_Test_Append_Only_Recovery()
    {
        const int initialBlocks = 500;
        var random = new Random(789);
        var originalBlocks = new Dictionary<long, byte[]>();

        _output.WriteLine("=== Append-Only Resilience Test ===");
        
        // Write initial blocks
        for (int i = 0; i < initialBlocks; i++)
        {
            var data = new byte[128];
            random.NextBytes(data);
            
            var block = new Block
            {
                Version = 1,
                Type = BlockType.Segment,
                Flags = 0,
                Encoding = PayloadEncoding.RawBytes,
                Timestamp = DateTime.UtcNow.Ticks,
                BlockId = 30000 + i,
                Payload = data
            };

            await _blockManager.WriteBlockAsync(block);
            originalBlocks[block.BlockId] = data;
        }

        _output.WriteLine($"Wrote {initialBlocks} original blocks");

        // Simulate updates by writing same BlockIds with new data
        var updatedBlocks = new Dictionary<long, byte[]>();
        var blocksToUpdate = originalBlocks.Keys.Take(50).ToList();
        
        _output.WriteLine($"\nUpdating {blocksToUpdate.Count} blocks (append-only)...");
        
        foreach (var blockId in blocksToUpdate)
        {
            var newData = new byte[256]; // Different size
            random.NextBytes(newData);
            
            var block = new Block
            {
                Version = 2, // Updated version
                Type = BlockType.Segment,
                Flags = 1, // Updated flag
                Encoding = PayloadEncoding.RawBytes,
                Timestamp = DateTime.UtcNow.Ticks,
                BlockId = blockId,
                Payload = newData
            };

            await _blockManager.WriteBlockAsync(block);
            updatedBlocks[blockId] = newData;
        }

        // Verify we can read the latest version
        _output.WriteLine("\nVerifying latest versions are returned...");
        foreach (var blockId in blocksToUpdate)
        {
            var result = await _blockManager.ReadBlockAsync(blockId);
            Assert.True(result.IsSuccess);
            Assert.Equal(2, result.Value.Version);
            Assert.Equal(1, result.Value.Flags);
            Assert.Equal(updatedBlocks[blockId], result.Value.Payload);
        }

        // File should contain both old and new versions
        var fileSize = new FileInfo(_testFile).Length;
        _output.WriteLine($"\nFile contains both versions:");
        _output.WriteLine($"- File size: {fileSize:N0} bytes");
        _output.WriteLine($"- This demonstrates append-only behavior");
        
        // Test recovery scenario
        _output.WriteLine("\nSimulating recovery by rebuilding index...");
        var stopwatch = Stopwatch.StartNew();
        
        // In a real scenario, this would happen after file corruption or crash
        var allLocations = _blockManager.GetBlockLocations();
        
        stopwatch.Stop();
        
        // Should have more locations than blocks due to duplicates
        Assert.True(allLocations.Count > initialBlocks);
        _output.WriteLine($"Recovery completed:");
        _output.WriteLine($"- Total block locations found: {allLocations.Count}");
        _output.WriteLine($"- Recovery time: {stopwatch.ElapsedMilliseconds}ms");
        _output.WriteLine($"- Original blocks are preserved for potential recovery");
    }

    [Fact]
    public async Task Performance_Concurrent_Access_Test()
    {
        const int threadCount = 10;
        const int blocksPerThread = 100;
        var random = new Random(999);

        _output.WriteLine($"=== Concurrent Access Performance Test ===");
        _output.WriteLine($"Threads: {threadCount}, Blocks per thread: {blocksPerThread}");

        var stopwatch = Stopwatch.StartNew();
        var tasks = new List<Task<List<long>>>();

        // Launch concurrent write operations
        for (int t = 0; t < threadCount; t++)
        {
            var threadId = t;
            var task = Task.Run(async () =>
            {
                var writtenIds = new List<long>();
                var threadRandom = new Random(threadId * 1000);

                for (int i = 0; i < blocksPerThread; i++)
                {
                    var data = new byte[512];
                    threadRandom.NextBytes(data);
                    
                    var block = new Block
                    {
                        Version = 1,
                        Type = BlockType.Segment,
                        Flags = (byte)threadId,
                        Encoding = PayloadEncoding.RawBytes,
                        Timestamp = DateTime.UtcNow.Ticks,
                        BlockId = 40000 + (threadId * 1000) + i,
                        Payload = data
                    };

                    var result = await _blockManager.WriteBlockAsync(block);
                    if (result.IsSuccess)
                        writtenIds.Add(block.BlockId);
                }

                return writtenIds;
            });

            tasks.Add(task);
        }

        // Wait for all threads to complete
        var allResults = await Task.WhenAll(tasks);
        stopwatch.Stop();

        var totalWritten = allResults.Sum(r => r.Count);
        var expectedTotal = threadCount * blocksPerThread;

        _output.WriteLine($"\nConcurrent Write Results:");
        _output.WriteLine($"- Total blocks written: {totalWritten}/{expectedTotal}");
        _output.WriteLine($"- Total time: {stopwatch.ElapsedMilliseconds:N0}ms");
        _output.WriteLine($"- Throughput: {totalWritten / (stopwatch.ElapsedMilliseconds / 1000.0):F2} blocks/second");
        
        Assert.Equal(expectedTotal, totalWritten);

        // Verify all blocks can be read
        _output.WriteLine("\nVerifying concurrent writes...");
        var verifyCount = 0;
        
        foreach (var blockIds in allResults)
        {
            foreach (var blockId in blockIds.Take(10)) // Sample verification
            {
                var result = await _blockManager.ReadBlockAsync(blockId);
                if (result.IsSuccess)
                    verifyCount++;
            }
        }

        _output.WriteLine($"Verified {verifyCount} blocks successfully written and readable");
    }

    [Fact]
    public async Task Performance_Large_File_Handling()
    {
        const int targetSizeMB = 100;
        const int bytesPerMB = 1024 * 1024;
        var random = new Random(555);
        var blockCount = 0;
        var totalBytes = 0L;

        _output.WriteLine($"=== Large File Performance Test (Target: {targetSizeMB}MB) ===");

        var stopwatch = Stopwatch.StartNew();

        // Write blocks until we reach target size
        while (totalBytes < targetSizeMB * bytesPerMB)
        {
            var dataSize = 10000 + random.Next(50000); // 10KB to 60KB blocks
            var data = new byte[dataSize];
            random.NextBytes(data);

            var block = new Block
            {
                Version = 1,
                Type = BlockType.Segment,
                Flags = 0,
                Encoding = PayloadEncoding.RawBytes,
                Timestamp = DateTime.UtcNow.Ticks,
                BlockId = 50000 + blockCount,
                Payload = data
            };

            var result = await _blockManager.WriteBlockAsync(block);
            if (result.IsSuccess)
            {
                totalBytes += dataSize + 61; // Data + overhead
                blockCount++;
            }

            if (blockCount % 100 == 0)
            {
                var currentMB = totalBytes / (double)bytesPerMB;
                _output.WriteLine($"Progress: {currentMB:F2}MB written ({blockCount} blocks)");
            }
        }

        stopwatch.Stop();
        var writeTime = stopwatch.ElapsedMilliseconds;

        var fileInfo = new FileInfo(_testFile);
        var actualSizeMB = fileInfo.Length / (double)bytesPerMB;

        _output.WriteLine($"\nLarge File Write Performance:");
        _output.WriteLine($"- Blocks written: {blockCount:N0}");
        _output.WriteLine($"- File size: {actualSizeMB:F2}MB");
        _output.WriteLine($"- Write time: {writeTime:N0}ms ({writeTime / 1000.0:F2} seconds)");
        _output.WriteLine($"- Write speed: {actualSizeMB / (writeTime / 1000.0):F2} MB/second");

        // Test random access in large file
        _output.WriteLine("\nTesting random access in large file...");
        var accessTimes = new List<double>();
        
        for (int i = 0; i < 50; i++)
        {
            var targetId = 50000 + random.Next(blockCount);
            var accessStopwatch = Stopwatch.StartNew();
            
            var result = await _blockManager.ReadBlockAsync(targetId);
            
            accessStopwatch.Stop();
            if (result.IsSuccess)
                accessTimes.Add(accessStopwatch.Elapsed.TotalMilliseconds);
        }

        _output.WriteLine($"Random access in {actualSizeMB:F2}MB file:");
        _output.WriteLine($"- Average access time: {accessTimes.Average():F3}ms");
        _output.WriteLine($"- Min access time: {accessTimes.Min():F3}ms");
        _output.WriteLine($"- Max access time: {accessTimes.Max():F3}ms");
    }

    public void Dispose()
    {
        _blockManager?.Dispose();
        
        if (File.Exists(_testFile))
        {
            try
            {
                var fileSize = new FileInfo(_testFile).Length;
                _output.WriteLine($"\nCleaning up test file ({fileSize:N0} bytes)");
                File.Delete(_testFile);
            }
            catch
            {
                // Best effort
            }
        }
    }
}