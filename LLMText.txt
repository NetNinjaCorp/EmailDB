\BlockManager.cs
using DragonHoard.InMemory;
using EmailDB.Format.Models;
using Microsoft.Extensions.Options;
using ProtoBuf;
using System.Security.Cryptography;
using System.Collections.Concurrent;

namespace EmailDB.Format;

public class BlockManager : IDisposable
{
    private readonly FileStream fileStream;
    private readonly object fileLock = new object();
    private long lastWrittenPosition;
    private readonly bool enableJournaling;
    private readonly string journalPath;
    private readonly ConcurrentDictionary<long, WeakReference<Block>> blockCache;
    private readonly ConcurrentDictionary<BlockType, List<long>> blockTypeIndex;
    private readonly Timer cacheCleanupTimer;
    private bool isDisposed;

    public class BlockManagerOptions
    {
        public int MaxRetries { get; set; } = 3;
        public int BufferSize { get; set; } = 8192;
        public bool EnableJournaling { get; set; } = true;
        public TimeSpan CacheCleanupInterval { get; set; } = TimeSpan.FromMinutes(5);
        public int MaxConcurrentOperations { get; set; } = 5;
        public TimeSpan RetryDelay { get; set; } = TimeSpan.FromMilliseconds(100);
        public int MaxCacheSize { get; set; } = 10000;
        public bool EnableBlockTypeIndexing { get; set; } = true;
    }

    private readonly BlockManagerOptions options;
    private readonly SemaphoreSlim concurrencyLimiter;

    public BlockManager(FileStream stream, BlockManagerOptions options = null)
    {
        if (stream == null)
            throw new ArgumentNullException(nameof(stream));

        if (!stream.CanRead || !stream.CanWrite)
            throw new ArgumentException("Stream must support both reading and writing", nameof(stream));

        this.options = options ?? new BlockManagerOptions();
        fileStream = stream;
        enableJournaling = this.options.EnableJournaling;
        journalPath = enableJournaling ? Path.ChangeExtension(stream.Name, ".journal") : null;
        lastWrittenPosition = stream.Length;
        blockCache = new ConcurrentDictionary<long, WeakReference<Block>>();
        blockTypeIndex = new ConcurrentDictionary<BlockType, List<long>>();
        concurrencyLimiter = new SemaphoreSlim(this.options.MaxConcurrentOperations);

        if (this.options.EnableBlockTypeIndexing)
        {
            InitializeBlockTypeIndex();
        }

        cacheCleanupTimer = new Timer(CleanupCache, null,
            this.options.CacheCleanupInterval,
            this.options.CacheCleanupInterval);

        if (enableJournaling && File.Exists(journalPath))
        {
            RecoverFromJournal();
        }
    }

    private void InitializeBlockTypeIndex()
    {
        foreach (BlockType type in Enum.GetValues(typeof(BlockType)))
        {
            blockTypeIndex[type] = new List<long>();
        }

        foreach (var (offset, block) in WalkBlocks())
        {
            if (blockTypeIndex.TryGetValue(block.Header.Type, out var offsetList))
            {
                lock (offsetList)
                {
                    offsetList.Add(offset);
                }
            }
        }
    }

    public IEnumerable<(long Offset, Block Block)> GetBlocksByType(BlockType type)
    {
        if (!options.EnableBlockTypeIndexing)
            throw new InvalidOperationException("Block type indexing is disabled");

        if (blockTypeIndex.TryGetValue(type, out var offsetList))
        {
            List<long> offsets;
            lock (offsetList)
            {
                offsets = offsetList.ToList(); // Create a copy to avoid locking during iteration
            }

            foreach (var offset in offsets)
            {
                var block = ReadBlock(offset);
                if (block != null)
                {
                    yield return (offset, block);
                }
            }
        }
    }

    public async Task<long> WriteBlockAsync(Block block, long? specificOffset = null,
        CancellationToken cancellationToken = default)
    {
        if (block == null)
            throw new ArgumentNullException(nameof(block));

        if (specificOffset.HasValue && specificOffset.Value < 0)
            throw new ArgumentOutOfRangeException(nameof(specificOffset));

        if (isDisposed)
            throw new ObjectDisposedException(nameof(BlockManager));

        await concurrencyLimiter.WaitAsync(cancellationToken);
        try
        {
            for (int attempt = 0; attempt < options.MaxRetries; attempt++)
            {
                try
                {
                    return await WriteBlockInternalAsync(block, specificOffset, cancellationToken);
                }
                catch (IOException ex) when (attempt < options.MaxRetries - 1)
                {
                    await Task.Delay(
                        TimeSpan.FromMilliseconds(options.RetryDelay.TotalMilliseconds * (attempt + 1)),
                        cancellationToken);
                }
            }

            throw new IOException($"Failed to write block after {options.MaxRetries} attempts");
        }
        finally
        {
            concurrencyLimiter.Release();
        }
    }

    private async Task<long> WriteBlockInternalAsync(Block block, long? specificOffset,
        CancellationToken cancellationToken)
    {
        lock (fileLock)
        {
            long position = specificOffset ?? lastWrittenPosition;

            if (enableJournaling)
            {
                WriteToJournal(block, position);
            }

            using (var ms = new MemoryStream())
            {
                block.Header.Checksum = 0;
                Serializer.SerializeWithLengthPrefix(ms, block, PrefixStyle.Base128);
                block.Header.Checksum = CalculateChecksum(ms.ToArray());
            }

            if (position == -1)
            {
                position = fileStream.Length;
            }

            fileStream.Position = position;

            using (var ms = new MemoryStream())
            {
                Serializer.SerializeWithLengthPrefix(ms, block, PrefixStyle.Base128);
                var data = ms.ToArray();
                fileStream.Write(data, 0, data.Length);
            }

            fileStream.Flush(true);

            if (!specificOffset.HasValue)
            {
                lastWrittenPosition = fileStream.Position;
            }

            // Update cache and index
            UpdateCacheAndIndex(position, block);

            if (enableJournaling)
            {
                ClearJournalEntry(position);
            }

            return position;
        }
    }

    private void UpdateCacheAndIndex(long position, Block block)
    {
        // Update cache with weak reference
        if (blockCache.Count >= options.MaxCacheSize)
        {
            CleanupCache(null);
        }
        blockCache[position] = new WeakReference<Block>(block);

        // Update block type index if enabled
        if (options.EnableBlockTypeIndexing &&
            blockTypeIndex.TryGetValue(block.Header.Type, out var offsetList))
        {
            lock (offsetList)
            {
                offsetList.Add(position);
            }
        }
    }

    public async Task<Block> ReadBlockAsync(long offset, CancellationToken cancellationToken = default)
    {
        if (offset < 0)
            throw new ArgumentOutOfRangeException(nameof(offset));

        if (isDisposed)
            throw new ObjectDisposedException(nameof(BlockManager));

        // Try cache first
        if (TryGetFromCache(offset, out var cachedBlock))
        {
            return cachedBlock;
        }

        await concurrencyLimiter.WaitAsync(cancellationToken);
        try
        {
            for (int attempt = 0; attempt < options.MaxRetries; attempt++)
            {
                try
                {
                    var block = await ReadBlockInternalAsync(offset, cancellationToken);
                    blockCache[offset] = new WeakReference<Block>(block);
                    return block;
                }
                catch (IOException ex) when (attempt < options.MaxRetries - 1)
                {
                    await Task.Delay(
                        TimeSpan.FromMilliseconds(options.RetryDelay.TotalMilliseconds * (attempt + 1)),
                        cancellationToken);
                }
            }

            throw new IOException($"Failed to read block after {options.MaxRetries} attempts");
        }
        finally
        {
            concurrencyLimiter.Release();
        }
    }

    private bool TryGetFromCache(long offset, out Block block)
    {
        block = null;
        return blockCache.TryGetValue(offset, out var weakRef) &&
               weakRef.TryGetTarget(out block);
    }

    private async Task<Block> ReadBlockInternalAsync(long offset, CancellationToken cancellationToken)
    {
        lock (fileLock)
        {
            if (offset >= fileStream.Length)
            {
                throw new ArgumentException($"Invalid offset: {offset}. File length: {fileStream.Length}");
            }

            fileStream.Position = offset;
            Block block;

            try
            {
                block = Serializer.DeserializeWithLengthPrefix<Block>(fileStream, PrefixStyle.Base128);
            }
            catch (Exception ex)
            {
                throw new InvalidDataException($"Failed to deserialize block at offset {offset}", ex);
            }

            if (block == null)
            {
                throw new InvalidDataException($"Failed to deserialize block at offset {offset}");
            }

            if (!ValidateBlock(block, offset))
            {
                if (enableJournaling)
                {
                    var recoveredBlock = TryRecoverBlockFromJournal(offset);
                    if (recoveredBlock != null)
                    {
                        return recoveredBlock;
                    }
                }
                throw new InvalidDataException($"Block validation failed at offset {offset}");
            }

            return block;
        }
    }

    private bool ValidateBlock(Block block, long offset)
    {
        uint storedChecksum = block.Header.Checksum;
        block.Header.Checksum = 0;

        using (var ms = new MemoryStream())
        {
            Serializer.SerializeWithLengthPrefix(ms, block, PrefixStyle.Base128);
            uint computedChecksum = CalculateChecksum(ms.ToArray());

            // Restore original checksum
            block.Header.Checksum = storedChecksum;

            return computedChecksum == storedChecksum;
        }
    }

    public IEnumerable<(long Offset, Block Block)> WalkBlocks()
    {
        if (isDisposed)
            throw new ObjectDisposedException(nameof(BlockManager));

        long currentOffset = 0;
        int errorCount = 0;
        const int MAX_ERRORS = 5;

        while (currentOffset < fileStream.Length)
        {
            var result = TryReadBlockAt(currentOffset);
            if (result == null)
            {
                errorCount++;
                if (errorCount >= MAX_ERRORS)
                {
                    throw new InvalidOperationException(
                        $"Too many errors encountered while walking blocks. Last offset: {currentOffset}");
                }
                currentOffset += options.BufferSize;
                continue;
            }

            errorCount = 0;
            yield return (currentOffset, result.Value.Block);
            currentOffset = result.Value.NextOffset;
        }
    }

    private (Block Block, long NextOffset)? TryReadBlockAt(long offset)
    {
        try
        {
            lock (fileLock)
            {
                fileStream.Position = offset;
                Block block = Serializer.DeserializeWithLengthPrefix<Block>(fileStream, PrefixStyle.Base128);

                if (block == null)
                    return null;

                if (!ValidateBlock(block, offset))
                {
                    if (enableJournaling)
                    {
                        var recoveredBlock = TryRecoverBlockFromJournal(offset);
                        if (recoveredBlock != null)
                        {
                            return (recoveredBlock, fileStream.Position);
                        }
                    }
                    return null;
                }

                return (block, fileStream.Position);
            }
        }
        catch (Exception ex)
        {
            // Log error if needed
            return null;
        }
    }

    private void WriteToJournal(Block block, long position)
    {
        if (!enableJournaling) return;

        var journalEntry = new JournalEntry
        {
            Position = position,
            Block = block,
            Timestamp = DateTimeOffset.UtcNow.ToUnixTimeMilliseconds()
        };

        using var journalStream = new FileStream(journalPath, FileMode.Append, FileAccess.Write, FileShare.None);
        using var ms = new MemoryStream();
        Serializer.SerializeWithLengthPrefix(ms, journalEntry, PrefixStyle.Base128);
        var data = ms.ToArray();
        journalStream.Write(data, 0, data.Length);
        journalStream.Flush(true);
    }

    private void ClearJournalEntry(long position)
    {
        if (!enableJournaling || !File.Exists(journalPath)) return;

        var entries = ReadJournalEntries()
            .Where(e => e.Position != position)
            .ToList();

        if (entries.Count == 0)
        {
            File.Delete(journalPath);
            return;
        }

        using var journalStream = new FileStream(journalPath, FileMode.Create, FileAccess.Write, FileShare.None);
        foreach (var entry in entries)
        {
            using var ms = new MemoryStream();
            Serializer.SerializeWithLengthPrefix(ms, entry, PrefixStyle.Base128);
            var data = ms.ToArray();
            journalStream.Write(data, 0, data.Length);
        }
        journalStream.Flush(true);
    }

    private Block TryRecoverBlockFromJournal(long position)
    {
        if (!enableJournaling || !File.Exists(journalPath)) return null;

        var entry = ReadJournalEntries()
            .Where(e => e.Position == position)
            .OrderByDescending(e => e.Timestamp)
            .FirstOrDefault();

        return entry?.Block;
    }

    private IEnumerable<JournalEntry> ReadJournalEntries()
    {
        if (!File.Exists(journalPath)) yield break;
        using var journalStream = new FileStream(journalPath, FileMode.Open, FileAccess.Read, FileShare.Read);
        while (journalStream.Position < journalStream.Length)
        {
            var entry = TryReadEntry(journalStream);
            if (entry != null)
            {
                yield return entry;
            }
        }
    }

    private JournalEntry TryReadEntry(FileStream journalStream)
    {
        try
        {
            return Serializer.DeserializeWithLengthPrefix<JournalEntry>(journalStream, PrefixStyle.Base128);
        }
        catch
        {
            // Skip corrupted entries
            return null;
        }
    }

    private void RecoverFromJournal()
    {
        if (!File.Exists(journalPath)) return;

        var recoveredEntries = new List<JournalEntry>();
        foreach (var entry in ReadJournalEntries())
        {
            try
            {
                WriteBlockInternal(entry.Block, entry.Position);
                recoveredEntries.Add(entry);
            }
            catch (Exception ex)
            {
                // Log recovery failure but continue with other entries
                Console.WriteLine($"Failed to recover block at position {entry.Position}: {ex.Message}");
            }
        }

        // Clear journal after successful recovery
        if (recoveredEntries.Any())
        {
            File.Delete(journalPath);
        }
    }

    private long WriteBlockInternal(Block block, long position)
    {
        using var ms = new MemoryStream();
        block.Header.Checksum = 0;
        Serializer.SerializeWithLengthPrefix(ms, block, PrefixStyle.Base128);
        block.Header.Checksum = CalculateChecksum(ms.ToArray());

        fileStream.Position = position;
        ms.Position = 0;
        ms.CopyTo(fileStream);
        fileStream.Flush(true);

        // Update cache and index
        UpdateCacheAndIndex(position, block);

        return fileStream.Position;
    }

    private void CleanupCache(object state)
    {
        if (isDisposed) return;

        var keysToRemove = new List<long>();
        foreach (var kvp in blockCache)
        {
            if (!kvp.Value.TryGetTarget(out _))
            {
                keysToRemove.Add(kvp.Key);
            }
        }

        foreach (var key in keysToRemove)
        {
            blockCache.TryRemove(key, out _);
        }

        // Additional cleanup for block type index if enabled
        if (options.EnableBlockTypeIndexing)
        {
            foreach (var type in blockTypeIndex.Keys)
            {
                if (blockTypeIndex.TryGetValue(type, out var offsetList))
                {
                    lock (offsetList)
                    {
                        offsetList.RemoveAll(offset => !IsValidOffset(offset));
                    }
                }
            }
        }
    }

    private bool IsValidOffset(long offset)
    {
        try
        {
            return offset >= 0 && offset < fileStream.Length;
        }
        catch
        {
            return false;
        }
    }

    private uint CalculateChecksum(byte[] data)
    {
        using var sha = SHA256.Create();
        byte[] hash = sha.ComputeHash(data);
        return BitConverter.ToUInt32(hash, 0);
    }

    // Synchronous versions of async methods for backward compatibility
    public long WriteBlock(Block block, long? specificOffset = null)
    {
        return WriteBlockAsync(block, specificOffset).GetAwaiter().GetResult();
    }

    public Block ReadBlock(long offset)
    {
        return ReadBlockAsync(offset).GetAwaiter().GetResult();
    }

    public void Dispose()
    {
        if (!isDisposed)
        {
            cacheCleanupTimer?.Dispose();
            concurrencyLimiter?.Dispose();
            blockCache.Clear();
            blockTypeIndex.Clear();
            isDisposed = true;
        }
    }

    [ProtoContract]
    private class JournalEntry
    {
        [ProtoMember(1)]
        public long Position { get; set; }

        [ProtoMember(2)]
        public Block Block { get; set; }

        [ProtoMember(3)]
        public long Timestamp { get; set; }
    }
}
\CacheManager.cs
using EmailDB.Format.Models;
using System.Collections.Concurrent;

namespace EmailDB.Format;

public class CacheManager : IDisposable
{
    private readonly BlockManager blockManager;
    private volatile HeaderContent cachedHeader;
    private readonly ConcurrentDictionary<string, (long Offset, FolderContent Content, DateTime LastAccess)> folderCache;
    private readonly ConcurrentDictionary<string, (MetadataContent Content, DateTime LastAccess)> metadataCache;
    private volatile FolderTreeContent cachedFolderTree;
    private readonly ReaderWriterLockSlim cacheLock;
    private readonly int maxCacheSize;
    private readonly TimeSpan cacheTimeout;
    private readonly Timer cacheCleanupTimer;
    private bool isDisposed;

    public CacheManager(BlockManager blockManager, int maxCacheSize = 1000, TimeSpan? cacheTimeout = null)
    {
        this.blockManager = blockManager ?? throw new ArgumentNullException(nameof(blockManager));
        this.maxCacheSize = maxCacheSize;
        this.cacheTimeout = cacheTimeout ?? TimeSpan.FromMinutes(30);

        folderCache = new ConcurrentDictionary<string, (long, FolderContent, DateTime)>();
        metadataCache = new ConcurrentDictionary<string, (MetadataContent, DateTime)>();
        cacheLock = new ReaderWriterLockSlim();

        // Start periodic cache cleanup
        cacheCleanupTimer = new Timer(CleanupCache, null, TimeSpan.FromMinutes(5), TimeSpan.FromMinutes(5));
       
    }

    public void LoadHeaderContent()
    {
        try
        {
            cacheLock.EnterWriteLock();
            try
            {
                var headerBlock = blockManager.ReadBlock(0);
                if (headerBlock?.Content is HeaderContent header)
                {
                    ValidateHeader(header);
                    cachedHeader = header;
                }
                else
                {
                    throw new InvalidDataException("Invalid or missing header block");
                }
            }
            finally
            {
                cacheLock.ExitWriteLock();
            }
        }
        catch (Exception ex)
        {
            throw new InvalidOperationException("Failed to load header content", ex);
        }
    }

    private void ValidateHeader(HeaderContent header)
    {
        if (header.FileVersion <= 0)
            throw new InvalidDataException("Invalid file version in header");

        if (header.FirstMetadataOffset < -1)
            throw new InvalidDataException("Invalid metadata offset in header");

        if (header.FirstFolderTreeOffset < -1)
            throw new InvalidDataException("Invalid folder tree offset in header");

        if (header.FirstCleanupOffset < -1)
            throw new InvalidDataException("Invalid cleanup offset in header");
    }

    public HeaderContent GetHeader()
    {
        ThrowIfDisposed();
        cacheLock.EnterReadLock();
        try
        {
            return cachedHeader ?? throw new InvalidOperationException("Header not loaded");
        }
        finally
        {
            cacheLock.ExitReadLock();
        }
    }

    public void UpdateHeader(HeaderContent header)
    {
        ThrowIfDisposed();
        if (header == null) throw new ArgumentNullException(nameof(header));

        ValidateHeader(header);

        cacheLock.EnterWriteLock();
        try
        {
            cachedHeader = header;
            var headerBlock = new Block
            {
                Header = new BlockHeader
                {
                    Type = BlockType.Header,
                    Timestamp = DateTimeOffset.UtcNow.ToUnixTimeSeconds(),
                    Version = 1
                },
                Content = header
            };
            blockManager.WriteBlock(headerBlock, 0);
        }
        finally
        {
            cacheLock.ExitWriteLock();
        }
    }

    public FolderContent GetCachedFolder(string folderName)
    {
        ThrowIfDisposed();
        if (string.IsNullOrWhiteSpace(folderName))
            throw new ArgumentException("Folder name cannot be null or empty", nameof(folderName));

        if (folderCache.TryGetValue(folderName, out var cachedFolder))
        {
            try
            {
                var block = blockManager.ReadBlock(cachedFolder.Offset);
                if (block?.Content is FolderContent folder && folder.Name == folderName)
                {
                    // Update last access time
                    folderCache.TryUpdate(folderName,
                        (cachedFolder.Offset, folder, DateTime.UtcNow),
                        cachedFolder);
                    return folder;
                }
            }
            catch (Exception ex)
            {
                // Log error if needed
                folderCache.TryRemove(folderName, out _);
            }
        }
        return null;
    }

    public void CacheFolder(string folderName, long offset, FolderContent folder)
    {
        ThrowIfDisposed();
        if (string.IsNullOrWhiteSpace(folderName))
            throw new ArgumentException("Folder name cannot be null or empty", nameof(folderName));
        if (folder == null)
            throw new ArgumentNullException(nameof(folder));
        if (offset < 0)
            throw new ArgumentException("Offset cannot be negative", nameof(offset));

        // Implement LRU eviction if cache is full
        if (folderCache.Count >= maxCacheSize)
        {
            var oldestEntry = folderCache
                .OrderBy(x => x.Value.LastAccess)
                .FirstOrDefault();

            if (!string.IsNullOrEmpty(oldestEntry.Key))
            {
                folderCache.TryRemove(oldestEntry.Key, out _);
            }
        }

        folderCache.AddOrUpdate(folderName,
            (offset, folder, DateTime.UtcNow),
            (_, _) => (offset, folder, DateTime.UtcNow));
    }

    public FolderTreeContent GetCachedFolderTree()
    {
        ThrowIfDisposed();
        cacheLock.EnterReadLock();
        try
        {
            if (cachedFolderTree != null)
            {
                return cachedFolderTree;
            }

            if (cachedHeader.FirstFolderTreeOffset != -1)
            {
                try
                {
                    var block = blockManager.ReadBlock(cachedHeader.FirstFolderTreeOffset);
                    if (block?.Content is FolderTreeContent tree)
                    {
                        cachedFolderTree = tree;
                        return tree;
                    }
                }
                catch (Exception ex)
                {
                    // Log error if needed
                }
            }
            return null;
        }
        finally
        {
            cacheLock.ExitReadLock();
        }
    }

    public void CacheFolderTree(FolderTreeContent tree)
    {
        ThrowIfDisposed();
        if (tree == null) throw new ArgumentNullException(nameof(tree));

        cacheLock.EnterWriteLock();
        try
        {
            cachedFolderTree = tree;
        }
        finally
        {
            cacheLock.ExitWriteLock();
        }
    }

    public MetadataContent GetCachedMetadata()
    {
        ThrowIfDisposed();
        if (cachedHeader.FirstMetadataOffset == -1)
        {
            return null;
        }

        var key = cachedHeader.FirstMetadataOffset.ToString();
        if (metadataCache.TryGetValue(key, out var cached))
        {
            metadataCache.TryUpdate(key,
                (cached.Content, DateTime.UtcNow),
                cached);
            return cached.Content;
        }

        try
        {
            var block = blockManager.ReadBlock(cachedHeader.FirstMetadataOffset);
            if (block?.Content is MetadataContent metadata)
            {
                metadataCache.TryAdd(key, (metadata, DateTime.UtcNow));
                return metadata;
            }
        }
        catch (Exception ex)
        {
            // Log error if needed
        }
        return null;
    }

    public void InvalidateCache()
    {
        ThrowIfDisposed();
        cacheLock.EnterWriteLock();
        try
        {
            folderCache.Clear();
            metadataCache.Clear();
            cachedFolderTree = null;
            LoadHeaderContent();
        }
        finally
        {
            cacheLock.ExitWriteLock();
        }
    }

    private void CleanupCache(object state)
    {
        if (isDisposed) return;

        var expirationTime = DateTime.UtcNow - cacheTimeout;

        // Clean up folder cache
        var expiredFolders = folderCache
            .Where(x => x.Value.LastAccess < expirationTime)
            .Select(x => x.Key)
            .ToList();

        foreach (var folder in expiredFolders)
        {
            folderCache.TryRemove(folder, out _);
        }

        // Clean up metadata cache
        var expiredMetadata = metadataCache
            .Where(x => x.Value.LastAccess < expirationTime)
            .Select(x => x.Key)
            .ToList();

        foreach (var key in expiredMetadata)
        {
            metadataCache.TryRemove(key, out _);
        }
    }

    private void ThrowIfDisposed()
    {
        if (isDisposed)
        {
            throw new ObjectDisposedException(nameof(CacheManager));
        }
    }

    public void Dispose()
    {
        if (!isDisposed)
        {
            isDisposed = true;
            cacheCleanupTimer?.Dispose();
            cacheLock?.Dispose();
            folderCache.Clear();
            metadataCache.Clear();
            cachedFolderTree = null;
            cachedHeader = null;
        }
    }
}
\EmailManager.cs
using MimeKit;
using Tenray.ZoneTree;
using EmailDB.Format.Models;
using EmailDB.Format.ZoneTree;
using System.Collections.Concurrent;
using ZoneTree.FullTextSearch.SearchEngines;

namespace EmailDB.Format;

public class EmailManager : IDisposable
{
    private readonly StorageManager storageManager;
    private readonly IZoneTree<EmailHashedID, EnhancedEmailContent> emailIndex;
    private readonly ConcurrentDictionary<string, EmailHashedID> emailCache;
    private readonly HashedSearchEngine<EmailHashedID> emailSearchEngine;
    private readonly FolderManager folderManager;
    private readonly SegmentManager segmentManager;
    private readonly object writeLock = new object();

    public EmailManager(StorageManager storageManager, FolderManager folderManager, SegmentManager segmentManager)
    {
        this.storageManager = storageManager ?? throw new ArgumentNullException(nameof(storageManager));
        this.folderManager = folderManager ?? throw new ArgumentNullException(nameof(folderManager));
        this.segmentManager = segmentManager ?? throw new ArgumentNullException(nameof(segmentManager));

        // Initialize ZoneTree for email indexing
        var factory = new EmailDBZoneTreeFactory<EmailHashedID, EnhancedEmailContent>(storageManager);

        if (!factory.CreateZoneTree("email_index")) { throw new Exception("Problem Creating Email Index"); }
        this.emailIndex = factory.OpenOrCreate();
        this.emailCache = new ConcurrentDictionary<string, EmailHashedID>();
       // this.emailSearchEngine = new HashedSearchEngine<EmailHashedID>();
    }

    public async Task<EmailHashedID> AddEmailAsync(string emlFilePath, string folderName)
    {
        if (string.IsNullOrWhiteSpace(emlFilePath))
            throw new ArgumentException("EML file path cannot be empty", nameof(emlFilePath));

        if (string.IsNullOrWhiteSpace(folderName))
            throw new ArgumentException("Folder name cannot be empty", nameof(folderName));

        // Read and parse the email
        var message = await ReadEmailFromFileAsync(emlFilePath);
        if (message == null)
            throw new InvalidOperationException("Failed to read email from file");

        lock (writeLock)
        {
            // Generate EmailHashedID
            var emailId = new EmailHashedID(message);

            // Check if email already exists
            if (emailIndex.TryGet(emailId, out var _))
                throw new InvalidOperationException("Email already exists in the system");

            // Create enhanced email content
            var enhancedContent = CreateEnhancedEmailContent(message);

            // Store in ZoneTree
            emailIndex.TryAdd(emailId, enhancedContent, out long OpIndex);

            // Add to folder
            folderManager.AddEmailToFolder(folderName, emailId).RunSynchronously();

            // Cache the email ID
            emailCache[message.MessageId] = emailId;

            return emailId;
        }
    }
    public async Task<EmailHashedID> AddEmailAsync(byte[] emlBytes, string folderName)
    {

        if (string.IsNullOrWhiteSpace(folderName))
            throw new ArgumentException("Folder name cannot be empty", nameof(folderName));

        // Read and parse the email
        var message = new MimeMessage(new MemoryStream(emlBytes));
        if (message == null)
            throw new InvalidOperationException("Failed to read email from file");

        lock (writeLock)
        {
            // Generate EmailHashedID
            var emailId = new EmailHashedID(message);

            // Check if email already exists
            if (emailIndex.TryGet(emailId, out var _))
                throw new InvalidOperationException("Email already exists in the system");

            // Create enhanced email content
            var enhancedContent = CreateEnhancedEmailContent(message);

            // Store in ZoneTree
            emailIndex.TryAdd(emailId, enhancedContent, out long OpIndex);

            // Add to folder
            folderManager.AddEmailToFolder(folderName, emailId).RunSynchronously();

            // Cache the email ID
            emailCache[message.MessageId] = emailId;

            return emailId;
        }
    }

    public async Task<EnhancedEmailContent> GetEmailContentAsync(EmailHashedID emailId)
    {
        if (emailIndex.TryGet(emailId, out var content))
            return content;

        throw new KeyNotFoundException("Email not found");
    }

    public async Task<byte[]> GetRawEmailAsync(EmailHashedID emailId)
    {
        if (emailIndex.TryGet(emailId, out var content))
        {
            return content.RawEmailContent;
        }
        else { return null; }
    }



    public async Task<List<EmailHashedID>> SearchEmailsAsync(string searchTerm, SearchField field)
    {
        var results = new List<EmailHashedID>();

        // Perform search through ZoneTree entries

        //// Use the search engine to find matching email IDs


        return results;
    }

    public async Task DeleteEmailAsync(EmailHashedID emailId, string folderName)
    {
        lock (writeLock)
        {
            // Remove from ZoneTree
            if (emailIndex.TryDelete(emailId, out var res))
            {

            }

            // Remove from folder           
            folderManager.RemoveEmailFromFolder(folderName, emailId);

            // Remove from cache
            foreach (var key in emailCache.Where(x => x.Value.Equals(emailId)))
            {
                emailCache.TryRemove(key.Key, out _);
            }
        }
    }

    public async Task MoveEmailAsync(EmailHashedID emailId, string sourceFolder, string targetFolder)
    {
        storageManager.MoveEmail(emailId, sourceFolder, targetFolder);
    }

    private async Task<MimeMessage> ReadEmailFromFileAsync(string emlFilePath)
    {
        using var stream = File.OpenRead(emlFilePath);
        return await MimeMessage.LoadAsync(stream);
    }

    private EnhancedEmailContent CreateEnhancedEmailContent(MimeMessage message)
    {
        return new EnhancedEmailContent
        {
            StrSubject = message.Subject ?? string.Empty,
            StrFrom = message.From.ToString() ?? string.Empty,
            StrTo = message.To.ToString() ?? string.Empty,
            StrCc = message.Cc.ToString() ?? string.Empty,
            StrBcc = message.Bcc.ToString() ?? string.Empty,
            Date = message.Date.DateTime,
            StrTextContent = GetTextContent(message),
            AttachmentCount = message.Attachments.Count(),
            ProcessedTime = DateTime.UtcNow,
            // Store the raw content if needed for full reconstruction
            RawEmailContent = GetRawContent(message)
        };
    }

    private string GetTextContent(MimeMessage message)
    {
        return message.TextBody ?? string.Empty;
    }

    private byte[] GetRawContent(MimeMessage message)
    {
        using var ms = new MemoryStream();
        message.WriteTo(ms);
        return ms.ToArray();
    }

    public void Dispose()
    {
        emailIndex?.Dispose();
        emailCache.Clear();
    }
}

public enum SearchField
{
    Subject,
    From,
    To,
    Content
}
\FolderManager.cs
using DragonHoard.InMemory;
using EmailDB.Format.Models;
using Microsoft.Extensions.Options;
using System.Text.RegularExpressions;

namespace EmailDB.Format;

public class FolderManager
{
    private readonly BlockManager blockManager;
    private readonly MetadataManager metadataManager;
    private readonly InMemoryCache inMemoryCache;
    private readonly object folderTreeLock = new object();
    private const char PathSeparator = '\\';

    public FolderManager(BlockManager blockManager, MetadataManager metadataManager)
    {
        this.blockManager = blockManager ?? throw new ArgumentNullException(nameof(blockManager));
        this.metadataManager = metadataManager ?? throw new ArgumentNullException(nameof(metadataManager));

        IEnumerable<IOptions<InMemoryCacheOptions>> options = new List<IOptions<InMemoryCacheOptions>>()
        {
            new OptionsWrapper<InMemoryCacheOptions>(new InMemoryCacheOptions
            {
                MaxCacheSize = 10000,
                ScanFrequency = TimeSpan.FromMinutes(5)
            })
        };
        inMemoryCache = new InMemoryCache(options);
    }

    // High-level folder operations
    public async Task CreateFolder(string path)
    {
        ValidatePath(path);

        var (parentPath, folderName) = SplitPath(path);
        var folderTree = await GetLatestFolderTree();

        // Check if path already exists
        if (folderTree.FolderIDs.ContainsKey(path))
            throw new InvalidOperationException($"Folder '{path}' already exists");

        // Get parent folder ID
        long parentFolderId = 0;
        if (!string.IsNullOrEmpty(parentPath))
        {
            if (!folderTree.FolderIDs.TryGetValue(parentPath, out parentFolderId))
                throw new InvalidOperationException($"Parent folder '{parentPath}' not found");
        }
        else if (folderTree.RootFolderId == -1)
        {
            // This will be the root folder
            parentFolderId = 0;
        }

        // Create new folder
        var folderId = GetNextFolderId(folderTree);
        var folder = new FolderContent
        {
            FolderId = folderId,
            ParentFolderId = parentFolderId,
            Name = path, // Store the full path as the name
            EmailIds = new List<EmailHashedID>()
        };

        // Write folder and update tree
        var offset = WriteFolder(folder);
        folderTree.FolderIDs[path] = folderId;
        folderTree.FolderOffsets[folderId] = offset;

        if (parentFolderId != 0)
            folderTree.FolderHierarchy[folderId.ToString()] = parentFolderId.ToString();
        else if (folderTree.RootFolderId == -1)
            folderTree.RootFolderId = folderId;

        WriteFolderTree(folderTree);
    }

    public async Task DeleteFolder(string path, bool deleteContents = false)
    {
        ValidatePath(path);
        var folder = await GetFolder(path);
        if (folder == null)
            return;

        var folderTree = await GetLatestFolderTree();

        // Get all subfolders that start with this path
        var subfolders = folderTree.FolderIDs
            .Where(x => x.Key.StartsWith(path + PathSeparator))
            .ToList();

        if (subfolders.Any())
            throw new InvalidOperationException("Cannot delete folder with subfolders");

        if (!deleteContents && folder.EmailIds.Any())
            throw new InvalidOperationException("Cannot delete non-empty folder without deleteContents flag");

        // Remove from folder tree
        folderTree.FolderIDs.Remove(path);
        folderTree.FolderOffsets.Remove(folder.FolderId);
        folderTree.FolderHierarchy.Remove(folder.FolderId.ToString());

        if (folderTree.RootFolderId == folder.FolderId)
            folderTree.RootFolderId = -1;

        WriteFolderTree(folderTree);
        InvalidateCache();
    }

    public async Task MoveFolder(string sourcePath, string targetParentPath)
    {
        ValidatePath(sourcePath);
        ValidatePath(targetParentPath);

        var sourceFolder = await GetFolder(sourcePath);
        if (sourceFolder == null)
            throw new InvalidOperationException($"Folder '{sourcePath}' not found");

        var folderTree = await GetLatestFolderTree();

        // Get target parent folder ID
        if (!folderTree.FolderIDs.TryGetValue(targetParentPath, out var targetParentId))
            throw new InvalidOperationException($"Target parent folder '{targetParentPath}' not found");

        // Check for circular reference
        var currentParent = targetParentId;
        while (currentParent != 0)
        {
            if (currentParent == sourceFolder.FolderId)
                throw new InvalidOperationException("Moving folder would create circular reference");

            var parentStr = currentParent.ToString();
            if (!folderTree.FolderHierarchy.TryGetValue(parentStr, out var nextParentStr))
                break;

            if (!long.TryParse(nextParentStr, out currentParent))
                break;
        }

        // Calculate new path
        var folderName = GetFolderName(sourcePath);
        var newPath = Combine(targetParentPath, folderName);

        // Check if new path already exists
        if (folderTree.FolderIDs.ContainsKey(newPath))
            throw new InvalidOperationException($"Target folder '{newPath}' already exists");

        // Update folder and its subfolders
        var oldPrefix = sourcePath + PathSeparator;
        var newPrefix = newPath + PathSeparator;
        var foldersToUpdate = folderTree.FolderIDs
            .Where(x => x.Key.StartsWith(oldPrefix) || x.Key == sourcePath)
            .ToList();

        foreach (var kvp in foldersToUpdate)
        {
            var oldPath = kvp.Key;
            var newFolderPath = oldPath == sourcePath ?
                newPath :
                newPrefix + oldPath.Substring(oldPrefix.Length);

            var folderToUpdate = await GetFolder(oldPath);
            folderToUpdate.Name = newFolderPath;

            if (oldPath == sourcePath)
                folderToUpdate.ParentFolderId = targetParentId;

            var newOffset = WriteFolder(folderToUpdate);

            folderTree.FolderIDs.Remove(oldPath);
            folderTree.FolderIDs[newFolderPath] = kvp.Value;
            folderTree.FolderOffsets[kvp.Value] = newOffset;
        }

        folderTree.FolderHierarchy[sourceFolder.FolderId.ToString()] = targetParentId.ToString();
        WriteFolderTree(folderTree);
    }

    public async Task<List<EmailHashedID>> GetEmails(string path)
    {
        var folder = await GetFolder(path);
        return folder?.EmailIds ?? new List<EmailHashedID>();
    }


    public async Task AddEmailToFolder(string path, EmailHashedID emailId)
    {
        var folder = await GetFolder(path);
        if (folder == null)
            throw new InvalidOperationException($"Folder '{path}' not found");

        if (!folder.EmailIds.Contains(emailId))
        {
            folder.EmailIds.Add(emailId);
            WriteFolder(folder);
        }
    }

    public async Task RemoveEmailFromFolder(string path, EmailHashedID emailId)
    {
        var folder = await GetFolder(path);
        if (folder == null)
            throw new InvalidOperationException($"Folder '{path}' not found");

        if (folder.EmailIds.Remove(emailId))
        {
            WriteFolder(folder);
        }
    }

    public async Task MoveEmail(EmailHashedID emailId, string sourcePath, string targetPath)
    {
        var source = await GetFolder(sourcePath);
        var target = await GetFolder(targetPath);

        if (source == null)
            throw new InvalidOperationException($"Source folder '{sourcePath}' not found");
        if (target == null)
            throw new InvalidOperationException($"Target folder '{targetPath}' not found");

        if (!source.EmailIds.Contains(emailId))
            throw new InvalidOperationException($"Email {emailId} not found in source folder");

        source.EmailIds.Remove(emailId);
        target.EmailIds.Add(emailId);

        WriteFolder(source);
        WriteFolder(target);
    }

    public async Task<List<string>> GetSubfolders(string path)
    {
        ValidatePath(path);
        var folderTree = await GetLatestFolderTree();
        var prefix = path + PathSeparator;

        return folderTree.FolderIDs
            .Where(x => x.Key.StartsWith(prefix))
            .Select(x => x.Key)
            .Where(x => x.Substring(prefix.Length).IndexOf(PathSeparator) == -1)
            .ToList();
    }

    // Path handling methods
    private void ValidatePath(string path)
    {
        if (string.IsNullOrWhiteSpace(path))
            throw new ArgumentException("Path cannot be empty");

        if (path.Contains("//"))
            throw new ArgumentException("Path cannot contain consecutive separators");

        if (path.EndsWith(PathSeparator))
            throw new ArgumentException("Path cannot end with separator");

        // Additional validation as needed
        var invalidChars = Path.GetInvalidPathChars()
            .Concat(new[] { '<', '>', ':', '"', '|', '?', '*' })
            .Where(c => c != PathSeparator);

        if (path.Any(c => invalidChars.Contains(c)))
            throw new ArgumentException("Path contains invalid characters");
    }

    private (string ParentPath, string FolderName) SplitPath(string path)
    {
        var lastSeparatorIndex = path.LastIndexOf(PathSeparator);
        if (lastSeparatorIndex == -1)
            return (string.Empty, path);

        var parentPath = path.Substring(0, lastSeparatorIndex);
        var folderName = path.Substring(lastSeparatorIndex + 1);
        return (parentPath, folderName);
    }

    private string GetFolderName(string path)
    {
        var (_, name) = SplitPath(path);
        return name;
    }

    private string Combine(string path1, string path2)
    {
        if (string.IsNullOrEmpty(path1)) return path2;
        return path1 + PathSeparator + path2;
    }

    // Internal helper methods
    internal async Task<FolderContent> GetFolder(string folderName)
    {
        if (string.IsNullOrWhiteSpace(folderName))
            throw new ArgumentException("Folder name cannot be empty", nameof(folderName));

        if (inMemoryCache.TryGetValue<FolderContent>(GetFolderCacheKey(folderName), out var cachedFolder))
            return cachedFolder;

        var folderTree = await GetLatestFolderTree();
        if (!folderTree.FolderIDs.TryGetValue(folderName, out var folderId))
            return null;

        if (!folderTree.FolderOffsets.TryGetValue(folderId, out var folderOffset))
            return null;

        var result = await blockManager.ReadBlockAsync(folderOffset);
        if (result?.Content is not FolderContent folder)
            return null;

        inMemoryCache.Set(GetFolderCacheKey(folderName), folder);
        return folder;
    }

    internal async Task<FolderTreeContent> GetLatestFolderTree()
    {
        if (inMemoryCache.TryGetValue<FolderTreeContent>(GetFolderTreeCacheKey(), out var cachedTree))
            return cachedTree;

        var folderTree = await metadataManager.GetFolderTree();
        lock (folderTreeLock)
        {          
            if (folderTree == null)
            {
                folderTree = new FolderTreeContent
                {
                    RootFolderId = -1,
                    FolderHierarchy = new Dictionary<string, string>(),
                    FolderIDs = new Dictionary<string, long>(),
                    FolderOffsets = new Dictionary<long, long>()
                };
            }

            inMemoryCache.Set(GetFolderTreeCacheKey(), folderTree);
            return folderTree;
        }
    }

    internal long WriteFolder(FolderContent folder)
    {
        var block = new Block
        {
            Header = new BlockHeader
            {
                Type = BlockType.Folder,
                Timestamp = DateTimeOffset.UtcNow.ToUnixTimeSeconds(),
                Version = 1
            },
            Content = folder
        };

        var offset = blockManager.WriteBlock(block);
        inMemoryCache.Set(GetFolderCacheKey(folder.Name), folder);
        return offset;
    }

    internal void WriteFolderTree(FolderTreeContent folderTree)
    {
        var block = new Block
        {
            Header = new BlockHeader
            {
                Type = BlockType.FolderTree,
                Timestamp = DateTimeOffset.UtcNow.ToUnixTimeSeconds(),
                Version = 1
            },
            Content = folderTree
        };

        var offset = blockManager.WriteBlock(block);
        metadataManager.UpdateFolderTreeOffset(offset);
        inMemoryCache.Set(GetFolderTreeCacheKey(), folderTree);
    }

    private long GetNextFolderId(FolderTreeContent folderTree)
    {
        return folderTree.FolderOffsets.Keys.DefaultIfEmpty(0).Max() + 1;
    }

    private string GetFolderCacheKey(string folderName)
    {
        return $"folder_{folderName}";
    }

    private string GetFolderTreeCacheKey()
    {
        return "folder_tree";
    }

    public void InvalidateCache()
    {
        inMemoryCache.Remove(GetFolderTreeCacheKey());
    }
}
\iStorageManager.cs
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace EmailDB.Format;
public interface IStorageManager : IDisposable
{
    void AddEmailToFolder(string folderName, byte[] emailContent);
    void MoveEmail(EmailHashedID emailId, string sourceFolder, string targetFolder);
    void DeleteEmail(EmailHashedID emailId, string folderName);
    void UpdateEmailContent(EmailHashedID emailId, byte[] newContent);
    void CreateFolder(string folderName, string parentFolderId = null);
    void DeleteFolder(string folderName, bool deleteEmails = false);
    void Compact(string outputPath);
    void InvalidateCache();
}
\MaintenanceManager.cs
using EmailDB.Format.Models;

namespace EmailDB.Format;

public class MaintenanceManager
{
    private readonly BlockManager blockManager;
    private readonly CacheManager cacheManager;
    private readonly FolderManager folderManager;
    private readonly SegmentManager segmentManager;

    public MaintenanceManager(BlockManager blockManager, CacheManager cacheManager,
                            FolderManager folderManager, SegmentManager segmentManager)
    {
        this.blockManager = blockManager;
        this.cacheManager = cacheManager;
        this.folderManager = folderManager;
        this.segmentManager = segmentManager;
    }

    public async void Compact(string outputPath)
    {
        using var outputStream = new FileStream(outputPath, FileMode.Create, FileAccess.ReadWrite, FileShare.None);
        var outputBlockManager = new BlockManager(outputStream);
        var outputCacheManager = new CacheManager(outputBlockManager);
        var metadataManager = new MetadataManager(outputBlockManager);

        // Initialize components for new file
        var outputFolderManager = new FolderManager(outputBlockManager, metadataManager);
        var outputEmailManager = new SegmentManager(outputBlockManager, outputCacheManager);

        // Get current state
        var folderTree = await folderManager.GetLatestFolderTree();
        var referencedSegments = new HashSet<long>();

        // Initialize new file
        InitializeNewFile(outputBlockManager, outputCacheManager);

        // Write folder tree
        outputFolderManager.WriteFolderTree(folderTree);

        //// Process each folder and its contents
        //foreach (var (_, block) in blockManager.WalkBlocks())
        //{
        //    if (block.Content is FolderContent folder)
        //    {
        //        if (folder.EmailIds.Count > 0)
        //        {
        //            // Write folder
        //            var folderOffset = outputFolderManager.WriteFolder(folder);

        //            // Track and write segments
        //            foreach (var emailId in folder.EmailIds)
        //            {
        //                if (referencedSegments.Add(emailId))
        //                {
        //                    var segment = segmentManager.GetLatestSegment(emailId);
        //                    if (segment != null)
        //                    {
        //                        var segmentOffset = outputEmailManager.WriteSegment(segment);

        //                        // Update metadata for new segment
        //                        outputEmailManager.UpdateMetadata(metadata =>
        //                        {
        //                            metadata.SegmentOffsets.Add(segmentOffset);
        //                            return metadata;
        //                        });
        //                    }
        //                }
        //            }

        //            // Update metadata for new folder
        //            outputEmailManager.UpdateMetadata(metadata =>
        //            {
        //                metadata.FolderOffsets[folder.Name] = folderOffset;
        //                return metadata;
        //            });
            //    }
            //}
        //}
    }

    private void InitializeNewFile(BlockManager blockManager, CacheManager cacheManager)
    {
        var headerBlock = new Block
        {
            Header = new BlockHeader
            {
                Type = BlockType.Header,
                Timestamp = DateTimeOffset.UtcNow.ToUnixTimeSeconds(),
                Version = 1
            },
            Content = new HeaderContent
            {
                FileVersion = 1,
                FirstMetadataOffset = -1,
                FirstFolderTreeOffset = -1,
                FirstCleanupOffset = -1
            }
        };

        blockManager.WriteBlock(headerBlock, 0);
        cacheManager.LoadHeaderContent();
    }

    public void PerformCleanup(long cleanupThreshold)
    {
        var metadata = cacheManager.GetCachedMetadata() ?? new MetadataContent();
        var currentTime = DateTimeOffset.UtcNow.ToUnixTimeSeconds();

        // Create cleanup record
        var cleanup = new CleanupContent
        {
            CleanupThreshold = cleanupThreshold,
            FolderTreeOffsets = new List<long>(),
            FolderOffsets = new Dictionary<string, List<long>>(),
            MetadataOffsets = new List<long>()
        };

        // Collect outdated blocks
        foreach (var (offset, block) in blockManager.WalkBlocks())
        {
            if (block.Header.Timestamp < cleanupThreshold)
            {
                switch (block.Content)
                {
                    case FolderTreeContent:
                        cleanup.FolderTreeOffsets.Add(offset);
                        break;
                    case FolderContent folder:
                        if (!cleanup.FolderOffsets.ContainsKey(folder.Name))
                            cleanup.FolderOffsets[folder.Name] = new List<long>();
                        cleanup.FolderOffsets[folder.Name].Add(offset);
                        break;
                    case MetadataContent:
                        cleanup.MetadataOffsets.Add(offset);
                        break;
                }
            }
        }

        // Write cleanup record
        var cleanupBlock = new Block
        {
            Header = new BlockHeader
            {
                Type = BlockType.Cleanup,
                Timestamp = currentTime,
                Version = 1
            },
            Content = cleanup
        };

        var cleanupOffset = blockManager.WriteBlock(cleanupBlock);

        // Update header with cleanup offset
        var header = cacheManager.GetHeader();
        header.FirstCleanupOffset = cleanupOffset;
        cacheManager.UpdateHeader(header);
    }
}
\MetadataManager.cs
using EmailDB.Format.Models;
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace EmailDB.Format;
public class MetadataManager
{
    private readonly BlockManager blockManager;
    private MetadataContent metadata;
    private readonly object metadataLock = new object();

    public MetadataManager(BlockManager blockManager)
    {
        this.blockManager = blockManager ?? throw new ArgumentNullException(nameof(blockManager));
        LoadMetadata();
    }

    private void LoadMetadata()
    {
        // Walk blocks to find latest metadata
        MetadataContent latest = null;
        foreach (var (_, block) in blockManager.WalkBlocks())
        {
            if (block.Content is MetadataContent metadataContent)
            {
                latest = metadataContent;
            }
        }

        metadata = latest ?? new MetadataContent();
    }

    public async Task<FolderTreeContent> GetFolderTree()
    {
        if (metadata.FolderTreeOffset == -1)
        {
            return null;
        }

        var result = await blockManager.ReadBlockAsync(metadata.FolderTreeOffset);
        if (result == null)
        {
            return null;
        }

        if (result.Content is not FolderTreeContent)
        {
            throw new InvalidOperationException("Folder tree offset does not point to a folder tree");
        }

        return (FolderTreeContent)result.Content;
    }

    public long WriteFolder(FolderContent folder)
    {
        var block = new Block
        {
            Header = new BlockHeader
            {
                Type = BlockType.Folder,
                Timestamp = DateTimeOffset.UtcNow.ToUnixTimeSeconds(),
                Version = 1
            },
            Content = folder
        };

        return blockManager.WriteBlock(block);
    }

    public void WriteFolderTree(FolderTreeContent folderTree)
    {
        var block = new Block
        {
            Header = new BlockHeader
            {
                Type = BlockType.FolderTree,
                Timestamp = DateTimeOffset.UtcNow.ToUnixTimeSeconds(),
                Version = 1
            },
            Content = folderTree
        };

        lock (metadataLock)
        {
            var offset = blockManager.WriteBlock(block);
            UpdateFolderTreeOffset(offset);
        }
    }

    public void UpdateFolderTreeOffset(long offset)
    {
        lock (metadataLock)
        {
            metadata.FolderTreeOffset = offset;
            WriteMetadata();
        }
    }

    private void WriteMetadata()
    {
        var block = new Block
        {
            Header = new BlockHeader
            {
                Type = BlockType.Metadata,
                Timestamp = DateTimeOffset.UtcNow.ToUnixTimeSeconds(),
                Version = 1
            },
            Content = metadata
        };

        blockManager.WriteBlock(block);
    }
}
\SegmentManager.cs
using EmailDB.Format.Models;

namespace EmailDB.Format;

public class SegmentManager
{
    private readonly BlockManager blockManager;
    private readonly CacheManager cacheManager;
    private readonly object metadataLock = new();

    public SegmentManager(BlockManager blockManager, CacheManager cacheManager)
    {
        this.blockManager = blockManager ?? throw new ArgumentNullException(nameof(blockManager));
        this.cacheManager = cacheManager ?? throw new ArgumentNullException(nameof(cacheManager));
    }

    public MetadataContent GetMetadata()
    {
        return cacheManager.GetCachedMetadata() ?? new MetadataContent();
    }

    public long WriteSegment(SegmentContent segment)
    {
        var block = new Block
        {
            Header = new BlockHeader
            {
                Type = BlockType.Segment,
                Timestamp = DateTimeOffset.UtcNow.ToUnixTimeSeconds(),
                Version = 1
            },
            Content = segment
        };
        // Update metadata with new segment offset
        lock (metadataLock)
        {
            var metadata = GetMetadata();
            var offset = blockManager.WriteBlock(block);
            metadata.SegmentOffsets[segment.FileName] = offset;
            UpdateMetadata(metadata);
            return offset;
        }
       
    }

    public Block ReadBlock(long offset)
    {
        return blockManager.ReadBlock(offset);
    }

    public void UpdateMetadata(MetadataContent metadata)
    {
        lock (metadataLock)
        {
            var block = new Block
            {
                Header = new BlockHeader
                {
                    Type = BlockType.Metadata,
                    Timestamp = DateTimeOffset.UtcNow.ToUnixTimeSeconds(),
                    Version = 1
                },
                Content = metadata
            };

            var offset = blockManager.WriteBlock(block);

            // Update header with new metadata offset
            var header = cacheManager.GetHeader();
            header.FirstMetadataOffset = offset;
            cacheManager.UpdateHeader(header);
        }
    }

    public void DeleteSegment(string path)
    {
        lock (metadataLock)
        {
            var metadata = GetMetadata();
            if (metadata.SegmentOffsets.TryGetValue(path, out var offset))
            {
                metadata.OutdatedOffsets.Add(offset);
                metadata.SegmentOffsets.Remove(path);
                UpdateMetadata(metadata);
            }
        }
    }

    public IEnumerable<(long Offset, Block Block)> WalkBlocks()
    {
        return blockManager.WalkBlocks();
    }

    public void Compact()
    {
        lock (metadataLock)
        {
            var metadata = GetMetadata();
            var validOffsets = new HashSet<long>(metadata.SegmentOffsets.Values);
            var outdatedOffsets = metadata.OutdatedOffsets.ToList();

            // Remove any outdated offsets that are still referenced
            outdatedOffsets.RemoveAll(offset => validOffsets.Contains(offset));

            metadata.OutdatedOffsets = outdatedOffsets;
            UpdateMetadata(metadata);
        }
    }

    public bool TryGetSegment(string path, out SegmentContent segment)
    {
        segment = null;
        var metadata = GetMetadata();

        if (metadata.SegmentOffsets.TryGetValue(path, out var offset))
        {
            var block = ReadBlock(offset);
            if (block?.Content is SegmentContent segmentContent)
            {
                segment = segmentContent;
                return true;
            }
        }

        return false;
    }

    public Dictionary<string, SegmentContent> GetAllSegments()
    {
        var result = new Dictionary<string, SegmentContent>();
        var metadata = GetMetadata();

        foreach (var kvp in metadata.SegmentOffsets)
        {
            var block = ReadBlock(kvp.Value);
            if (block?.Content is SegmentContent segment)
            {
                result[kvp.Key] = segment;
            }
        }

        return result;
    }

    public long GetSegmentOffset(string path)
    {
        var metadata = GetMetadata();
        return metadata.SegmentOffsets.TryGetValue(path, out var offset) ? offset : -1;
    }

    public bool IsSegmentOutdated(long offset)
    {
        var metadata = GetMetadata();
        return metadata.OutdatedOffsets.Contains(offset);
    }

    public void UpdateSegmentPath(string oldPath, string newPath)
    {
        lock (metadataLock)
        {
            var metadata = GetMetadata();
            if (metadata.SegmentOffsets.TryGetValue(oldPath, out var offset))
            {
                metadata.SegmentOffsets.Remove(oldPath);
                metadata.SegmentOffsets[newPath] = offset;
                UpdateMetadata(metadata);
            }
        }
    }

    public void CleanupOutdatedSegments()
    {
        lock (metadataLock)
        {
            var metadata = GetMetadata();
            var validOffsets = new HashSet<long>(metadata.SegmentOffsets.Values);
            metadata.OutdatedOffsets.RemoveAll(offset => validOffsets.Contains(offset));
            UpdateMetadata(metadata);
        }
    }
}
\StorageManager.cs
using EmailDB.Format.Models;
using EmailDB.Format;

public class StorageManager : IStorageManager
{
    private readonly string filePath;
    private readonly FileStream fileStream;
    public readonly BlockManager blockManager;
    public readonly CacheManager cacheManager;
    public readonly MetadataManager metadataManager;
    public readonly FolderManager folderManager;
    public readonly SegmentManager segmentManager;
    public readonly EmailManager emailManager;
    public readonly MaintenanceManager maintenanceManager;

    public StorageManager(string path, bool createNew = false)
    {
        filePath = path;
        var mode = createNew ? FileMode.Create : FileMode.OpenOrCreate;
        fileStream = new FileStream(path, mode, FileAccess.ReadWrite, FileShare.None);

        // Initialize components
        blockManager = new BlockManager(fileStream);
        cacheManager = new CacheManager(blockManager);
        metadataManager = new MetadataManager(blockManager);
        folderManager = new FolderManager(blockManager, metadataManager);
        if (createNew)
        {
            InitializeNewFile();
        }
        else
        {
            cacheManager.LoadHeaderContent();
        }
      
        segmentManager = new SegmentManager(blockManager, cacheManager);
        emailManager = new EmailManager(this, folderManager, segmentManager);
        maintenanceManager = new MaintenanceManager(blockManager, cacheManager, folderManager, segmentManager);

        if (createNew)
        {
            InitializeNewFile();
        }
        else
        {
            cacheManager.LoadHeaderContent();
        }
    }

    public void InitializeNewFile()
    {
        var headerBlock = new Block
        {
            Header = new BlockHeader
            {
                Type = BlockType.Header,
                Timestamp = DateTimeOffset.UtcNow.ToUnixTimeSeconds(),
                Version = 1
            },
            Content = new HeaderContent
            {
                FileVersion = 1,
                FirstMetadataOffset = -1,
                FirstFolderTreeOffset = -1,
                FirstCleanupOffset = -1
            }
        };

        blockManager.WriteBlock(headerBlock, 0);
        cacheManager.LoadHeaderContent();

        // Initialize empty folder tree
        folderManager.WriteFolderTree(new FolderTreeContent());
    }

    public async void AddEmailToFolder(string folderName, byte[] emailContent)
    {
        // The StorageManager now just coordinates between EmailManager and FolderManager
        var emailId = await emailManager.AddEmailAsync(emailContent,folderName);
        folderManager.AddEmailToFolder(folderName, emailId);
    }

    public void UpdateEmailContent(EmailHashedID emailId, byte[] newContent)
    {
        throw new NotImplementedException();
    }

    public async void MoveEmail(EmailHashedID emailId, string sourceFolder, string targetFolder)
    {
        // Coordinate the move operation between managers       
        await emailManager.MoveEmailAsync(emailId, sourceFolder, targetFolder);
    }

    public async void DeleteEmail(EmailHashedID emailId, string folderName)
    {
        // Coordinate deletion between managers
        await emailManager.MoveEmailAsync(emailId, folderName, "Deleted Items");
    }

    public async void CreateFolder(string folderName, string parentFolderId = null)
    {
        if (parentFolderId == null)
        {
            folderManager.CreateFolder(folderName);

        }
        else
        {
            var parentFolder = await folderManager.GetFolder(parentFolderId);
            if (parentFolder == null)
            {
                throw new InvalidOperationException("Parent folder does not exist");
            }
            folderManager.CreateFolder($"{parentFolderId}\\{folderName}");
        }
    }

    public async void DeleteFolder(string folderName, bool deleteEmails = false)
    {
        if (deleteEmails)
        {
            var folder = folderManager.GetFolder(folderName).Result;
            if (folder != null)
            {
                foreach (var emailId in folder.EmailIds)
                {
                    await emailManager.MoveEmailAsync(emailId, folderName, "Deleted Items");
                }
            }
        }
        folderManager.DeleteFolder(folderName, deleteEmails);
    }

    public HeaderContent GetHeader()
    {
        return cacheManager.GetHeader();
    }

    // These methods should be internal and only used by other managers
    internal Block ReadBlock(long offset)
    {
        return blockManager.ReadBlock(offset);
    }

    internal long WriteBlock(Block block, long? specificOffset = null)
    {
        return blockManager.WriteBlock(block, specificOffset);
    }

    internal IEnumerable<(long Offset, Block Block)> WalkBlocks()
    {
        return blockManager.WalkBlocks();
    }

    public void Compact(string outputPath)
    {
        lock (fileStream)
        {
            maintenanceManager.Compact(outputPath);
        }
    }

    public void InvalidateCache()
    {
        cacheManager.InvalidateCache();
    }

    public void Dispose()
    {
        emailManager?.Dispose();
        fileStream?.Dispose();
    }


}
\ZonetreeSegmentIO.cs
using EmailDB.Format.Models;

namespace EmailDB.Format;

// ZoneTree IO handler for segment files
public class ZoneTreeSegmentIO : IDisposable
{
    private readonly string basePath;
    private readonly Dictionary<string, FileStream> segmentStreams = new();
    private readonly object lockObj = new object();

    public ZoneTreeSegmentIO(string basePath)
    {
        this.basePath = basePath;
        Directory.CreateDirectory(basePath);
    }

    public void WriteSegment(SegmentContent segment)
    {
        var fileName = GetSegmentFileName(segment.SegmentId);
        lock (lockObj)
        {
            if (!segmentStreams.TryGetValue(fileName, out var stream))
            {
                stream = new FileStream(Path.Combine(basePath, fileName),
                    FileMode.OpenOrCreate, FileAccess.ReadWrite, FileShare.None);
                segmentStreams[fileName] = stream;
            }

            segment.FileOffset = stream.Length;
            segment.ContentLength = segment.SegmentData.Length;
            segment.FileName = fileName;

            stream.Position = segment.FileOffset;
            stream.Write(segment.SegmentData);
            stream.Flush(true);
        }
    }

    public byte[] ReadSegment(SegmentContent segment)
    {
        lock (lockObj)
        {
            if (!segmentStreams.TryGetValue(segment.FileName, out var stream))
            {
                stream = new FileStream(Path.Combine(basePath, segment.FileName),
                    FileMode.Open, FileAccess.Read, FileShare.Read);
                segmentStreams[segment.FileName] = stream;
            }

            stream.Position = segment.FileOffset;
            var buffer = new byte[segment.ContentLength];
            stream.Read(buffer, 0, segment.ContentLength);
            return buffer;
        }
    }

    private string GetSegmentFileName(long segmentId)
    {
        return $"segment_{segmentId / 1000:D3}.dat";
    }

    public void Dispose()
    {
        foreach (var stream in segmentStreams.Values)
        {
            stream.Dispose();
        }
        segmentStreams.Clear();
    }
}
\Block.cs
using ProtoBuf;
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace EmailDB.Format.Models;

[ProtoContract]
public class Block
{
    [ProtoMember(1)]
    public BlockHeader Header { get; set; }

    [ProtoMember(2)]
    public BlockContent Content { get; set; }
}
\BlockContent.cs
using ProtoBuf;
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace EmailDB.Format.Models;
[ProtoContract]
[ProtoInclude(100, typeof(HeaderContent))]
[ProtoInclude(101, typeof(FolderTreeContent))]
[ProtoInclude(102, typeof(FolderContent))]
[ProtoInclude(103, typeof(SegmentContent))]
[ProtoInclude(104, typeof(MetadataContent))]
[ProtoInclude(105, typeof(CleanupContent))]
[ProtoInclude(106, typeof(WALContent))]
public class BlockContent
{
    // Header content


}
\BlockHeader.cs
using ProtoBuf;
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace EmailDB.Format.Models;
[ProtoContract]
public class BlockHeader
{
    [ProtoMember(20)]
    public BlockType Type { get; set; }

    [ProtoMember(21)]
    public long NextBlockOffset { get; set; } = -1;

    [ProtoMember(22)]
    public long Timestamp { get; set; }

    [ProtoMember(23)]
    public uint Version { get; set; } = 1;

    [ProtoMember(24)]
    public uint Checksum { get; set; }
}
\BlockType.cs
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace EmailDB.Format.Models;
public enum BlockType
{
    Header = 1,
    FolderTree = 2,
    Folder = 3,
    Segment = 4,
    Metadata = 5,
    Cleanup = 6,
    WAL = 7
}
\CleanupContent.cs
using ProtoBuf;
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace EmailDB.Format.Models;
[ProtoContract]
public class CleanupContent : BlockContent
{
    [ProtoMember(1000)]
    public List<long> FolderTreeOffsets { get; set; } = new();

    [ProtoMember(1001)]
    public Dictionary<string, List<long>> FolderOffsets { get; set; } = new();

    [ProtoMember(1002)]
    public List<long> MetadataOffsets { get; set; } = new();

    [ProtoMember(1003)]
    public long CleanupThreshold { get; set; }
}
\EmailHashedID.cs
using System;
using System.Text;
using DZen.Security.Cryptography;
using MimeKit;
using SimpleBase;
using Tenray.ZoneTree.Comparers;
using Tenray.ZoneTree.Serializers;

public struct EmailHashedID : IComparable<EmailHashedID>, IEquatable<EmailHashedID>, ISerializer<EmailHashedID>, IRefComparer<EmailHashedID>
{
    // Store the full 256 bits (32 bytes) of the SHA3-256 hash
    private readonly ulong _part1; // First 8 bytes
    private readonly ulong _part2; // Second 8 bytes
    private readonly ulong _part3; // Third 8 bytes
    private readonly ulong _part4; // Fourth 8 bytes

    public ulong Part1 => _part1;
    public ulong Part2 => _part2;
    public ulong Part3 => _part3;
    public ulong Part4 => _part4;

    // Define a static default instance
    public static readonly EmailHashedID Empty = new EmailHashedID(new byte[32]);

    // Property to check if this is a default/empty instance
    public bool IsEmpty => _part1 == 0 && _part2 == 0 && _part3 == 0 && _part4 == 0;

    // Constructor for database reconstruction
    public EmailHashedID(ulong part1, ulong part2, ulong part3, ulong part4)
    {
        _part1 = part1;
        _part2 = part2;
        _part3 = part3;
        _part4 = part4;
    }

    public EmailHashedID()
    {
        _part1 = 0;
        _part2 = 0;
        _part3 = 0;
        _part4 = 0;
    }
    public EmailHashedID(byte[] hash)
    {
        if (hash == null || hash.Length != 32)
            throw new ArgumentException("Hash must be exactly 32 bytes (SHA3-256).", nameof(hash));

        _part1 = BitConverter.ToUInt64(hash, 0);
        _part2 = BitConverter.ToUInt64(hash, 8);
        _part3 = BitConverter.ToUInt64(hash, 16);
        _part4 = BitConverter.ToUInt64(hash, 24);
    }

    public EmailHashedID(string MessageID, long EmailTime, string EmailSubject, string EmailFrom, string EmailTo)
    {
        using var sha3 = SHA3.Create();
        var hash = sha3.ComputeHash(Encoding.UTF8.GetBytes(
            MessageID + EmailTime + EmailSubject + EmailFrom + EmailTo));

        _part1 = BitConverter.ToUInt64(hash, 0);
        _part2 = BitConverter.ToUInt64(hash, 8);
        _part3 = BitConverter.ToUInt64(hash, 16);
        _part4 = BitConverter.ToUInt64(hash, 24);
    }

    public EmailHashedID(MimeMessage message)
    {
        using var sha3 = SHA3.Create();
        var hash = sha3.ComputeHash(Encoding.UTF8.GetBytes(
            message.MessageId +
            message.Date.ToUnixTimeMilliseconds() +
            message.Subject +
            message.From.ToString() +
            message.To.ToString()));

        _part1 = BitConverter.ToUInt64(hash, 0);
        _part2 = BitConverter.ToUInt64(hash, 8);
        _part3 = BitConverter.ToUInt64(hash, 16);
        _part4 = BitConverter.ToUInt64(hash, 24);
    }

    public byte[] GetBytes()
    {
        var bytes = new byte[32];
        BitConverter.TryWriteBytes(bytes.AsSpan(0, 8), _part1);
        BitConverter.TryWriteBytes(bytes.AsSpan(8, 8), _part2);
        BitConverter.TryWriteBytes(bytes.AsSpan(16, 8), _part3);
        BitConverter.TryWriteBytes(bytes.AsSpan(24, 8), _part4);
        return bytes;
    }

    public string ToBase32String()
    {
        return Base32.ZBase32.Encode(GetBytes());
    }

    public static EmailHashedID FromBase32String(string base32)
    {
        var bytes = Base32.ZBase32.Decode(base32);
        if (bytes.Length != 32)
            throw new ArgumentException("Invalid base32 string length", nameof(base32));
        return new EmailHashedID(bytes);
    }

    public override bool Equals(object obj)
    {
        return obj is EmailHashedID other && Equals(other);
    }

    public bool Equals(EmailHashedID other)
    {
        return _part1 == other._part1 &&
               _part2 == other._part2 &&
               _part3 == other._part3 &&
               _part4 == other._part4;
    }

    public override int GetHashCode()
    {
        unchecked
        {
            int hash = 17;
            hash = (hash * 31) + _part1.GetHashCode();
            hash = (hash * 31) + _part2.GetHashCode();
            hash = (hash * 31) + _part3.GetHashCode();
            hash = (hash * 31) + _part4.GetHashCode();
            return hash;
        }
    }

    public int CompareTo(EmailHashedID other)
    {
        int comparison = _part1.CompareTo(other._part1);
        if (comparison != 0) return comparison;

        comparison = _part2.CompareTo(other._part2);
        if (comparison != 0) return comparison;

        comparison = _part3.CompareTo(other._part3);
        if (comparison != 0) return comparison;

        return _part4.CompareTo(other._part4);
    }

    public override string ToString()
    {
        return ToBase32String();
    }

    public static bool TryParse(string input, out EmailHashedID result)
    {
        try
        {
            result = FromBase32String(input);
            return true;
        }
        catch
        {
            result = default;
            return false;
        }
    }

    public bool TryFormat(Span<char> destination, out int charsWritten, ReadOnlySpan<char> format, IFormatProvider provider)
    {
        var str = ToBase32String();
        if (destination.Length < str.Length)
        {
            charsWritten = 0;
            return false;
        }
        str.CopyTo(destination);
        charsWritten = str.Length;
        return true;
    }

    public EmailHashedID Deserialize(Memory<byte> bytes)
    {
        return new EmailHashedID(bytes.ToArray());
    }

    public Memory<byte> Serialize(in EmailHashedID entry)
    {
        return GetBytes();
    }

    public int Compare(in EmailHashedID x, in EmailHashedID y)
    {
        return x.CompareTo(y);
    }
}
\EnhancedEmailContent.cs
using ProtoBuf;
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;
using Tenray.ZoneTree.Serializers;

[ProtoContract]
public class EnhancedEmailContent : ISerializer<EnhancedEmailContent>
{
    [ProtoMember(1)]
    public byte[] Subject { get; set; }

    public string StrSubject
    {
        get => Subject is null || Subject.Length == 0
            ? string.Empty
            : Encoding.UTF8.GetString(Subject);
        set
        {
            if (string.IsNullOrEmpty(value))
            {
                // Store an empty array instead of throwing an exception
                Subject = Array.Empty<byte>();
            }
            else
            {
                Subject = Encoding.UTF8.GetBytes(value);
            }
        }
    }

    [ProtoMember(2)]
    public byte[] From { get; set; }

    public string StrFrom
    {
        get => From is null || From.Length == 0
            ? string.Empty
            : Encoding.UTF8.GetString(From);
        set
        {
            if (string.IsNullOrEmpty(value))
            {
                From = Array.Empty<byte>();
            }
            else
            {
                From = Encoding.UTF8.GetBytes(value);
            }
        }
    }

    [ProtoMember(3)]
    public byte[] To { get; set; }

    public string StrTo
    {
        get => To is null || To.Length == 0
            ? string.Empty
            : Encoding.UTF8.GetString(To);
        set
        {
            if (string.IsNullOrEmpty(value))
            {
                To = Array.Empty<byte>();
            }
            else
            {
                To = Encoding.UTF8.GetBytes(value);
            }
        }
    }

    [ProtoMember(4)]
    public byte[] Cc { get; set; }

    public string StrCc
    {
        get => Cc is null || Cc.Length == 0
            ? string.Empty
            : Encoding.UTF8.GetString(Cc);
        set
        {
            if (string.IsNullOrEmpty(value))
            {
                Cc = Array.Empty<byte>();
            }
            else
            {
                Cc = Encoding.UTF8.GetBytes(value);
            }
        }
    }

    [ProtoMember(5)]
    public byte[] Bcc { get; set; }

    public string StrBcc
    {
        get => Bcc is null || Bcc.Length == 0
            ? string.Empty
            : Encoding.UTF8.GetString(Bcc);
        set
        {
            if (string.IsNullOrEmpty(value))
            {
                Bcc = Array.Empty<byte>();
            }
            else
            {
                Bcc = Encoding.UTF8.GetBytes(value);
            }
        }
    }

    [ProtoMember(6)]
    public DateTime Date { get; set; }

    [ProtoMember(7)]
    public byte[] TextContent { get; set; }

    public string StrTextContent
    {
        get => TextContent is null || TextContent.Length == 0
            ? string.Empty
            : Encoding.UTF8.GetString(TextContent);
        set
        {
            if (string.IsNullOrEmpty(value))
            {
                TextContent = Array.Empty<byte>();
            }
            else
            {
                TextContent = Encoding.UTF8.GetBytes(value);
            }
        }
    }

    [ProtoMember(8)]
    public byte[] AttachmentContent { get; set; }

    public string StrAttachmentContent
    {
        get => AttachmentContent is null || AttachmentContent.Length == 0
            ? string.Empty
            : Encoding.UTF8.GetString(AttachmentContent);
        set
        {
            if (string.IsNullOrEmpty(value))
            {
                AttachmentContent = Array.Empty<byte>();
            }
            else
            {
                AttachmentContent = Encoding.UTF8.GetBytes(value);
            }
        }
    }

    [ProtoMember(9)]
    public DateTime ProcessedTime { get; set; }

    [ProtoMember(10)]
    public int AttachmentCount { get; set; }
    [ProtoMember(11)]
    public byte[] RawEmailContent { get; set; }

    public EnhancedEmailContent Deserialize(Memory<byte> bytes)
    {
        return ProtoBuf.Serializer.Deserialize<EnhancedEmailContent>(bytes.Span);

    }

    public Memory<byte> Serialize(in EnhancedEmailContent entry)
    {
        var memStream = new MemoryStream();
        ProtoBuf.Serializer.Serialize(memStream, entry);
        return memStream.ToArray();
    }
}
\FolderContent.cs
using ProtoBuf;
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace EmailDB.Format.Models;
[ProtoContract]
public class FolderContent : BlockContent
{
    [ProtoMember(2000)]
    public long FolderId { get; set; }

    [ProtoMember(2001)]
    public long ParentFolderId { get; set; }

    [ProtoMember(2002)]
    public string Name { get; set; }

    [ProtoMember(2003)]
    public List<EmailHashedID> EmailIds { get; set; } = new();
}
\FolderTreeContent.cs
using ProtoBuf;
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace EmailDB.Format.Models;
[ProtoContract]
public class FolderTreeContent : BlockContent
{
    [ProtoMember(2500)]
    public long RootFolderId { get; set; }

    [ProtoMember(2501)]
    public Dictionary<string, string> FolderHierarchy { get; set; } = new();

    [ProtoMember(2502)]
    public Dictionary<string, long> FolderIDs { get; set; } = new();

    [ProtoMember(2503)]
    public Dictionary<long, long> FolderOffsets { get; set; } = new();

}

\HeaderContent.cs
using ProtoBuf;
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace EmailDB.Format.Models;
[ProtoContract]
public class HeaderContent : BlockContent
{
    [ProtoMember(3000)]
    public int FileVersion { get; set; }

    [ProtoMember(3001)]
    public long FirstMetadataOffset { get; set; }

    [ProtoMember(3002)]
    public long FirstFolderTreeOffset { get; set; }

    [ProtoMember(3003)]
    public long FirstCleanupOffset { get; set; }
}

\MetadataContent.cs
using ProtoBuf;
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace EmailDB.Format.Models;
[ProtoContract]
public class MetadataContent : BlockContent
{
    [ProtoMember(3500)]
    public long WALOffset { get; set; } = -1;
    [ProtoMember(3501)]
    public long FolderTreeOffset { get; set; } = -1;

    [ProtoMember(3502)]
    public Dictionary<string, long> SegmentOffsets { get; set; } = new();

    [ProtoMember(3503)]
    public List<long> OutdatedOffsets { get; set; } = new();

    [ProtoMember(3599)]
    public byte[] UpdateBuffer { get; set; } = new byte[10 * 1024 * 1024]; // 10MB buffer for updates
    
}

\SegmentContent.cs
using ProtoBuf;
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace EmailDB.Format.Models;
[ProtoContract]
public class SegmentContent : BlockContent
{
    [ProtoMember(4000)]
    public long SegmentId { get; set; }

    [ProtoMember(4001)]
    public byte[] SegmentData { get; set; }

    // New fields for ZoneTree integration
    [ProtoMember(4002)]
    public string FileName { get; set; }  // Name of the physical file containing this segment

    [ProtoMember(4003)]
    public long FileOffset { get; set; }  // Offset within the file where the segment data begins

    [ProtoMember(4004)]
    public int ContentLength { get; set; }  // Length of the email content in bytes

    [ProtoMember(4005)]
    public long SegmentTimestamp { get; set; }  // When this segment version was created

    [ProtoMember(4006)]
    public bool IsDeleted { get; set; }  // Soft deletion flag

    [ProtoMember(4007)]
    public uint Version { get; set; }  // Version number for this segment

    [ProtoMember(4008)]
    public Dictionary<string, string> Metadata { get; set; } = new();  // Optional metadata for the segment

    // Computed property to help with segment file organization
    public long SegmentFileGroup => SegmentId / 1000;
}

\WALContent.cs
using EmailDB.Format.ZoneTree;
using ProtoBuf;
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace EmailDB.Format.Models;
[ProtoContract]
public class WALContent : BlockContent
{
    [ProtoMember(5000)]
    public Dictionary<string, List<WALEntry>> Entries { get; set; } = new();

    [ProtoMember(5001)]
    public long NextWALOffset { get; set; } = -1;

    [ProtoMember(5002)]
    public Dictionary<string, long> CategoryOffsets { get; set; } = new();
}

[ProtoContract]
public class WALEntry
{
    [ProtoMember(1)]
    public byte[] SerializedKey { get; set; }

    [ProtoMember(2)]
    public byte[] SerializedValue { get; set; }

    [ProtoMember(3)]
    public long OpIndex { get; set; }

    [ProtoMember(4)]
    public string Category { get; set; }

    [ProtoMember(5)]
    public long SegmentId { get; set; }
}
\EmailDBHashedSearchEngine.cs
//using System;
//using System.Collections.Generic;
//using System.Linq;
//using System.Text;
//using System.Threading.Tasks;
//using Tenray.ZoneTree.Comparers;
//using ZoneTree.FullTextSearch.Hashing;
//using ZoneTree.FullTextSearch.Tokenizer;

//namespace EmailDB.Format.ZoneTree;

//public class EmailDBHashedSearchEngine<TRecord> : IDisposable
//    where TRecord : unmanaged
//{
//    /// <summary>
//    /// Gets the index used by the search engine to store and retrieve records.
//    /// </summary>
//    public readonly EmailDBIndexOfRecord<TRecord, ulong> Index;

//    /// <summary>
//    /// The tokenizer used to split text into word slices for hashing.
//    /// </summary>
//    readonly IWordTokenizer WordTokenizer;

//    /// <summary>
//    /// The hash code generator used to generate hash codes for tokens.
//    /// </summary>
//    readonly IHashCodeGenerator HashCodeGenerator;



//    /// <summary>
//    /// Initializes a new instance of the <see cref="HashedSearchEngine{TRecord}"/> class.
//    /// </summary>
//    /// <param name="dataPath">The path to the data storage, defaulting to "data".</param>
//    /// <param name="useSecondaryIndex">Indicates whether a secondary index should be used to perform faster deletion.</param>
//    /// <param name="recordComparer">The comparer used to manage references to records.</param>
//    /// <param name="wordTokenizer">The tokenizer used to split words. If null, a default tokenizer is used.</param>
//    /// <param name="hashCodeGenerator">The hash code generator used to generate hash codes for the tokens. If null, a default generator is used.</param>
//    /// <param name="blockCacheLifeTimeInMilliseconds">Defines the life time of cached blocks. Default is 1 minute.</param>
//    public EmailDBHashedSearchEngine(
//        StorageManager storageManager,
//        IWordTokenizer wordTokenizer = null,
//        IRefComparer<TRecord> recordComparer = null,
//        IHashCodeGenerator hashCodeGenerator = null,
//        long blockCacheLifeTimeInMilliseconds = 60_000)
//    {
//        HashCodeGenerator = hashCodeGenerator ?? new DefaultHashCodeGenerator();
//        Index = new(
//            storageManager,
//            recordComparer,
//            new UInt64ComparerAscending(),            
//            blockCacheLifeTimeInMilliseconds: blockCacheLifeTimeInMilliseconds);
//        WordTokenizer =
//            wordTokenizer ??
//            new WordTokenizer(hashCodeGenerator: HashCodeGenerator);
//    }

//    bool isDisposed;

//    public void Dispose()
//    {
//        if (isDisposed) return;
//        isDisposed = true;
//        Index.IsReadOnly = true;
//        Index.WaitForBackgroundThreads();
//        Index.Dispose();
//    }
//}

\EmailDBIndexOfRecord.cs
//using global::ZoneTree.FullTextSearch.Model;
//using global::ZoneTree.FullTextSearch.Search;
//using global::ZoneTree.FullTextSearch;
//using System.Threading;
//using Tenray.ZoneTree;
//using Tenray.ZoneTree.Comparers;
//using Tenray.ZoneTree.PresetTypes;
//using Tenray.ZoneTree.Serializers;
//using ZoneTree.FullTextSearch.Model;
//using ZoneTree.FullTextSearch.QueryLanguage;
//using ZoneTree.FullTextSearch.Search;
//using Tenray.ZoneTree.Options;

//namespace EmailDB.Format.ZoneTree;


///// <summary>
///// Represents an index structure for managing records associated with hashed tokens,
///// with support for an optional secondary index and token-based searching.
///// </summary>
///// <typeparam name="TRecord">The type of the records managed by the index. Must be an unmanaged type.</typeparam>
///// <typeparam name="TToken">The type of the tokens used for indexing. Must be an unmanaged type.</typeparam>
//public sealed class EmailDBIndexOfRecord<TRecord, TToken>
//    : IDisposable
//    where TRecord : unmanaged
//    where TToken : unmanaged
//{

//    bool isDropped;

//    bool isDisposed;

//    readonly SearchOnIndexOfTokenRecordPreviousToken<TRecord, TToken>
//        searchAlgorithm;

//    readonly AdvancedSearchOnIndexOfTokenRecordPreviousToken<TRecord, TToken>
//        advancedSearchAlgorithm;

//    /// <summary>
//    /// Gets the primary zone tree used to store and retrieve records by token and previous token.
//    /// </summary>
//    public readonly IZoneTree<
//        CompositeKeyOfTokenRecordPrevious<TRecord, TToken>,
//        byte> ZoneTree1;

//    /// <summary>
//    /// Gets the maintainer for managing the primary zone tree, including background tasks.
//    /// </summary>
//    public readonly IMaintainer Maintainer1;


//    /// <summary>
//    /// Gets the ref comparer of record.
//    /// </summary>
//    public IRefComparer<TRecord> RecordComparer { get; }

//    /// <summary>
//    /// Gets the ref comparer of token.
//    /// </summary>
//    public IRefComparer<TToken> TokenComparer { get; }

//    /// <summary>
//    /// Gets or sets a value indicating whether the index is read-only.
//    /// When set to true, both the primary and secondary zone trees (if applicable) become read-only.
//    /// </summary>
//    public bool IsReadOnly
//    {
//        get => ZoneTree1.IsReadOnly;
//        set
//        {
//            ZoneTree1.IsReadOnly = value;
//        }
//    }

//    /// <summary>
//    /// Returns true if the index is dropped, otherwise false.
//    /// </summary>
//    public bool IsIndexDropped { get => isDropped; }

//    /// <summary>
//    /// Initializes a new instance of the <see cref="IndexOfTokenRecordPreviousToken{TRecord, TToken}"/> class,
//    /// with the option to configure primary and secondary zone trees.
//    /// </summary>
//    /// <param name="recordComparer">The comparer of record.</param>
//    /// <param name="tokenComparer">The comparer of token.</param>      
//    /// <param name="configure1">Optional configuration action for the primary zone tree.</param>
//    /// <param name="blockCacheLifeTimeInMilliseconds">Defines the life time of cached blocks. Default is 1 minute.</param>
//    public EmailDBIndexOfRecord(StorageManager storageManager,
//        IRefComparer<TRecord> recordComparer = null,
//        IRefComparer<TToken> tokenComparer = null,
//        Action<EmailDBZoneTreeFactory<
//                CompositeKeyOfTokenRecordPrevious<TRecord, TToken>,
//                byte>> configure1 = null,
//        long blockCacheLifeTimeInMilliseconds = 60_000)
//    {        

//        if (recordComparer == null)
//            recordComparer = ComponentsForKnownTypes.GetComparer<TRecord>();
//        if (tokenComparer == null)
//            tokenComparer = ComponentsForKnownTypes.GetComparer<TToken>();
//        var factory1 = new EmailDBZoneTreeFactory<CompositeKeyOfTokenRecordPrevious<TRecord, TToken>, byte>(storageManager, true, CompressionMethod.LZ4)
//            .SetIsDeletedDelegate(
//                (in CompositeKeyOfTokenRecordPrevious<TRecord, TToken> key, in byte value) => value == 1)
//            .SetMarkValueDeletedDelegate((ref byte x) => x = 1)
//            .SetKeySerializer(new StructSerializer<CompositeKeyOfTokenRecordPrevious<TRecord, TToken>>())
//            .SetComparer(
//                new CompositeKeyOfTokenRecordPreviousComparer<TRecord, TToken>(
//                    recordComparer,
//                    tokenComparer));

//        configure1?.Invoke(factory1);

//        ZoneTree1 = factory1.OpenOrCreate();
//        Maintainer1 = ZoneTree1.CreateMaintainer();
//        Maintainer1.InactiveBlockCacheCleanupInterval = TimeSpan.FromSeconds(30);
//        Maintainer1.BlockCacheLifeTime =
//            TimeSpan.FromMilliseconds(blockCacheLifeTimeInMilliseconds);
//        Maintainer1.EnableJobForCleaningInactiveCaches = true;
//        RecordComparer = recordComparer;
//        TokenComparer = tokenComparer;
//        //searchAlgorithm = new(this.ZoneTree1);
//        //advancedSearchAlgorithm = new(this);
//    }

//    /// <summary>
//    /// Throws an exception if the index has been dropped, preventing further operations on a dropped index.
//    /// </summary>
//    public void ThrowIfIndexIsDropped()
//    {
//        if (isDropped) throw new Exception($"{nameof(
//            EmailDBIndexOfRecord<TRecord, TToken>)} is dropped.");
//    }

//    /// <summary>
//    /// Evicts data from memory to disk in both primary and secondary zone trees.
//    /// </summary>
//    public void EvictToDisk()
//    {
//        ThrowIfIndexIsDropped();
//        Maintainer1.EvictToDisk();
//    }

//    /// <summary>
//    /// Attempts to cancel any background threads associated with maintenance tasks for both zone trees.
//    /// </summary>
//    public void TryCancelBackgroundThreads()
//    {
//        ThrowIfIndexIsDropped();
//        Maintainer1.TryCancelBackgroundThreads();
//    }

//    /// <summary>
//    /// Waits for all background threads associated with maintenance tasks to complete for both zone trees.
//    /// </summary>
//    public void WaitForBackgroundThreads()
//    {
//        ThrowIfIndexIsDropped();
//        Maintainer1.WaitForBackgroundThreads();
//    }

//    /// <summary>
//    /// Drops the index by canceling and waiting for background threads, and then destroying the zone trees.
//    /// </summary>
//    public void Drop()
//    {
//        ThrowIfIndexIsDropped();
//        TryCancelBackgroundThreads();
//        WaitForBackgroundThreads();
//        isDropped = true;
//        IsReadOnly = true;
//        ZoneTree1.Maintenance.Drop();
//        ZoneTree1.Dispose();
//    }

//    /// <summary>
//    /// Upserts a record in the primary zone tree, and optionally in the secondary zone tree if enabled.
//    /// </summary>
//    /// <param name="token">The token associated with the record.</param>
//    /// <param name="record">The record to be upserted.</param>
//    /// <param name="previousToken">The token that precedes the current token in the record.</param>
//    public void UpsertRecord(TToken token, TRecord record, TToken previousToken)
//    {
//        ThrowIfIndexIsDropped();
//        ZoneTree1.Upsert(new CompositeKeyOfTokenRecordPrevious<TRecord, TToken>()
//        {
//            Token = token,
//            Record = record,
//            PreviousToken = previousToken
//        }, 0);
//    }

//    /// <summary>
//    /// Deletes a record from the primary zone tree, and optionally from the secondary zone tree if a secondary index is enabled.
//    /// </summary>
//    /// <param name="token">The token associated with the record to delete.</param>
//    /// <param name="record">The record to be deleted.</param>
//    /// <param name="previousToken">The token that precedes the current token in the record.</param>
//    public void DeleteRecord(TToken token, TRecord record, TToken previousToken)
//    {
//        ThrowIfIndexIsDropped();
//        ZoneTree1.ForceDelete(new CompositeKeyOfTokenRecordPrevious<TRecord, TToken>()
//        {
//            Token = token,
//            Record = record,
//            PreviousToken = previousToken
//        });

//    }

//    /// <summary>
//    /// Deletes a record from the index without using the secondary index.
//    /// </summary>
//    /// <param name="record">The record to delete.</param>
//    /// <returns>The number of entries deleted.</returns>
//    long DeleteRecordWithoutInvertedIndex(TRecord record)
//    {
//        using var iterator1 = ZoneTree1.CreateIterator(
//            IteratorType.NoRefresh,
//            contributeToTheBlockCache: false);
//        var deletedEntries = 0L;
//        var recordComparer = RecordComparer;
//        while (iterator1.Next())
//        {
//            var key = iterator1.CurrentKey;
//            if (recordComparer.AreNotEqual(key.Record, record)) continue;
//            ZoneTree1.ForceDelete(key);
//            ++deletedEntries;
//        }
//        return deletedEntries;
//    }

//    /// <summary>
//    /// Deletes a record from the index, including from the secondary index if enabled.
//    /// </summary>
//    /// <param name="record">The record to delete.</param>
//    /// <returns>The number of entries deleted.</returns>
//    public long DeleteRecord(TRecord record)
//    {
//        ThrowIfIndexIsDropped();
//        return DeleteRecordWithoutInvertedIndex(record);     
//    }

//    /// <summary>
//    /// Searches the index for records that match the specified tokens, with optional support for facets, token order respect, and pagination.
//    /// </summary>
//    /// <param name="tokens">
//    /// A read-only span of tokens that the records must contain. This parameter is mandatory unless facets are provided.
//    /// The tokens are logically grouped using "AND", meaning all tokens must be present in the matching records.
//    /// If both the tokens span and the facets span are empty, the result will be an empty array, as searching without tokens and facets is not supported.
//    /// Tokens can be empty if facets are provided; in this case, the search will be based solely on the facets.
//    /// To retrieve records without specific search tokens or facets, consider fetching them from the actual record source instead of using the search index.
//    /// </param>
//    /// <param name="firstLookAt">
//    /// An optional token that the search will prioritize when searching. 
//    /// If not specified, the first token in the tokens span is used.
//    /// </param>
//    /// <param name="respectTokenOrder">
//    /// A boolean indicating whether the search should respect the order of tokens in the record.
//    /// If true, the records must contain the tokens in the specified order.
//    /// </param>
//    /// <param name="facets">
//    /// An optional read-only span of tokens that can be used to filter the search results.
//    /// If any facets are provided, records must contain at least one of these facet tokens to be included in the results.
//    /// If the span is empty or not provided, no facet filtering is applied, and all matching records are returned regardless of facet values.
//    /// </param>
//    /// <param name="skip">
//    /// The number of matching records to skip in the result set, useful for pagination.
//    /// Defaults to 0.
//    /// </param>
//    /// <param name="limit">
//    /// The maximum number of records to return, useful for limiting the result set size.
//    /// Defaults to 0, which indicates no limit.
//    /// </param>
//    /// <param name="cancellationToken">
//    /// A token to monitor for cancellation requests. This allows the search operation to be canceled if necessary.
//    /// </param>
//    /// <returns>
//    /// An array of records that match the specified tokens and facets, respecting the token order if specified.
//    /// The array may be empty if no matching records are found.
//    /// </returns>
//    public TRecord[] SimpleSearch(
//        ReadOnlySpan<TToken> tokens,
//        TToken? firstLookAt = null,
//        bool respectTokenOrder = true,
//        ReadOnlySpan<TToken> facets = default,
//        int skip = 0,
//        int limit = 0,
//        CancellationToken cancellationToken = default)
//    {
//        return searchAlgorithm
//            .Search(tokens, firstLookAt, respectTokenOrder, facets, skip, limit, cancellationToken);
//    }

//    /// <summary>
//    /// Performs a search based on the specified query and returns the matching records.
//    /// </summary>
//    /// <param name="query">The search query to execute.</param>
//    /// <param name="cancellationToken">
//    /// A token to monitor for cancellation requests. This allows the search operation to be canceled if necessary.
//    /// </param>
//    /// <returns>An array of records that match the search criteria.</returns>
//    public TRecord[] Search(
//        SearchQuery<TToken> query,
//        CancellationToken cancellationToken = default)
//    {
//        return advancedSearchAlgorithm.Search(query, cancellationToken);
//    }

//    /// <summary>
//    /// Disposes the resources used by the index.
//    /// </summary>
//    public void Dispose()
//    {
//        if (isDisposed) return;
//        isDisposed = true;
//        Maintainer1.WaitForBackgroundThreads();
//        Maintainer1.Dispose();
//        ZoneTree1.Dispose();
//    }
//}

\FileStreamProvider.cs
using EmailDB.Format.Models;
using EmailDB.Format;
using Tenray.ZoneTree.AbstractFileStream;

public class EmailDBFileStreamProvider : IFileStreamProvider
{
    private readonly SegmentManager segmentManager;
    private readonly Dictionary<string, EmailDBFileStream> openStreams;
    private readonly object lockObj = new();

    public EmailDBFileStreamProvider(SegmentManager segmentManager)
    {
        this.segmentManager = segmentManager;
        this.openStreams = new Dictionary<string, EmailDBFileStream>();
    }

    public IFileStream CreateFileStream(string path, FileMode mode, FileAccess access,
        FileShare share, int bufferSize = 4096, FileOptions options = FileOptions.None)
    {
        lock (lockObj)
        {
            if (openStreams.TryGetValue(path, out var existingStream))
            {
                if (existingStream.CanBeReused(access))
                    return existingStream;

                existingStream.Dispose();
                openStreams.Remove(path);
            }

            var stream = new EmailDBFileStream(segmentManager, path, mode, access);
            openStreams[path] = stream;
            return stream;
        }
    }

    public void DeleteFile(string path)
    {
        lock (lockObj)
        {
            if (openStreams.TryGetValue(path, out var stream))
            {
                stream.Dispose();
                openStreams.Remove(path);
            }

            segmentManager.DeleteSegment(path); 
           
        }
    }

    public bool FileExists(string path)
    {
        var metadata = segmentManager.GetMetadata();
        return metadata.SegmentOffsets.ContainsKey(path);
    }

    // Simple IFileStreamProvider implementation methods
    public string CombinePaths(string path1, string path2) => Path.Combine(path1, path2);
    public void CreateDirectory(string path) { } // No-op as we use virtual paths
    public bool DirectoryExists(string path) => true; // Virtual paths always exist
    public void DeleteDirectory(string path, bool recursive)
    {
        var metadata = segmentManager.GetMetadata();
        var pathsToDelete = metadata.SegmentOffsets.Keys
            .Where(p => p.StartsWith(path))
            .ToList();

        foreach (var p in pathsToDelete)
        {
            DeleteFile(p);
        }
    }
    public IReadOnlyList<string> GetDirectories(string path) => new List<string>();
    public DurableFileWriter GetDurableFileWriter() => new DurableFileWriter(this);

    public string ReadAllText(string path)
    {
        throw new NotImplementedException();
    }

    public byte[] ReadAllBytes(string path)
    {
        throw new NotImplementedException();
    }

    public void Replace(string sourceFileName, string destinationFileName, string destinationBackupFileName)
    {
        throw new NotImplementedException();
    }
}

public class EmailDBFileStream : IFileStream
{
    private readonly SegmentManager segmentManager;
    private readonly string path;
    private readonly FileAccess access;
    private readonly List<SegmentContent> segments;
    private long position;
    private bool isDisposed;
    private readonly object writeLock = new();

    public EmailDBFileStream(SegmentManager segmentManager, string path,
        FileMode mode, FileAccess access)
    {
        this.segmentManager = segmentManager;
        this.path = path;
        this.access = access;
        this.segments = new List<SegmentContent>();

        InitializeStream(mode);
    }

    private void InitializeStream(FileMode mode)
    {
        var metadata = segmentManager.GetMetadata();

        // Load existing segment if it exists
        if (metadata.SegmentOffsets.TryGetValue(path, out var offset))
        {
            var block = segmentManager.ReadBlock(offset);
            if (block?.Content is SegmentContent segment)
            {
                segments.Add(segment);
            }
        }

        switch (mode)
        {
            case FileMode.Create:
            case FileMode.Truncate:
                segments.Clear();
                break;
            case FileMode.CreateNew:
                if (segments.Any())
                    throw new IOException("File already exists");
                break;
            case FileMode.Open:
                if (!segments.Any())
                    throw new FileNotFoundException();
                break;
            case FileMode.Append:
                position = segments.Sum(s => s.ContentLength);
                break;
        }
    }

    public void Write(byte[] buffer, int offset, int count)
    {
        if (!CanWrite)
            throw new NotSupportedException("Stream does not support writing");

        if (isDisposed)
            throw new ObjectDisposedException(nameof(EmailDBFileStream));

        lock (writeLock)
        {
            var segment = new SegmentContent
            {
                SegmentData = new byte[count],
                FileOffset = position,
                ContentLength = count,
                SegmentTimestamp = DateTimeOffset.UtcNow.ToUnixTimeSeconds(),
                Version = 1
            };

            Buffer.BlockCopy(buffer, offset, segment.SegmentData, 0, count);

            // Write segment and update metadata
            var segmentOffset = segmentManager.WriteSegment(segment);
            var metadata = segmentManager.GetMetadata();

            // If there was a previous segment, mark it as outdated
            if (metadata.SegmentOffsets.TryGetValue(path, out var oldOffset))
            {
                metadata.OutdatedOffsets.Add(oldOffset);
            }

            metadata.SegmentOffsets[path] = segmentOffset;
            segmentManager.UpdateMetadata(metadata);

            segments.Clear();
            segments.Add(segment);
            position += count;
        }
    }

    public int Read(byte[] buffer, int offset, int count)
    {
        if (!CanRead)
            throw new NotSupportedException("Stream does not support reading");

        if (isDisposed)
            throw new ObjectDisposedException(nameof(EmailDBFileStream));

        if (!segments.Any() || position >= segments[0].ContentLength)
            return 0;

        var segment = segments[0];
        var available = segment.ContentLength - position;
        var toRead = Math.Min(count, available);

        Buffer.BlockCopy(segment.SegmentData, (int)position, buffer, offset, (int)toRead);
        position += toRead;
        return (int)toRead;
    }

    // Basic IFileStream implementation
    public bool CanRead => access.HasFlag(FileAccess.Read);
    public bool CanWrite => access.HasFlag(FileAccess.Write);
    public bool CanSeek => true;
    public long Length => segments.Sum(s => s.ContentLength);
    public long Position
    {
        get => position;
        set
        {
            if (value < 0)
                throw new ArgumentOutOfRangeException(nameof(value));
            position = value;
        }
    }

    public string FilePath => throw new NotImplementedException();

    public bool CanTimeout => false;

    public int ReadTimeout
    {
        get => throw new InvalidOperationException("Timeouts are not supported");
        set => throw new InvalidOperationException("Timeouts are not supported");
    }

    public int WriteTimeout
    {
        get => throw new InvalidOperationException("Timeouts are not supported");
        set => throw new InvalidOperationException("Timeouts are not supported");
    }

    public bool CanBeReused(FileAccess requiredAccess) =>
        !isDisposed && (access == requiredAccess || access == FileAccess.ReadWrite);

    public void Dispose()
    {
        if (!isDisposed)
        {
            isDisposed = true;
            segments.Clear();
        }
    }

    public void Flush() { }
    public long Seek(long offset, SeekOrigin origin)
    {
        switch (origin)
        {
            case SeekOrigin.Begin:
                Position = offset;
                break;
            case SeekOrigin.Current:
                Position += offset;
                break;
            case SeekOrigin.End:
                Position = Length + offset;
                break;
        }
        return Position;
    }
    public void SetLength(long value) { }

    public IAsyncResult BeginRead(byte[] buffer, int offset, int count, AsyncCallback callback, object state)
    {
        var task = ReadAsync(buffer, offset, count);
        if (callback != null)
        {
            task.ContinueWith(t => callback(t), TaskScheduler.Default);
        }
        return task;
    }

    public IAsyncResult BeginWrite(byte[] buffer, int offset, int count, AsyncCallback callback, object state)
    {
        var task = WriteAsync(buffer, offset, count);
        if (callback != null)
        {
            task.ContinueWith(t => callback(t), TaskScheduler.Default);
        }
        return task;
    }

    public void Close()
    {
        Dispose();
    }


    public void CopyTo(Stream destination, int bufferSize)
    {
        if (destination == null)
            throw new ArgumentNullException(nameof(destination));
        if (bufferSize <= 0)
            throw new ArgumentOutOfRangeException(nameof(bufferSize));

        var buffer = new byte[bufferSize];
        int read;
        while ((read = Read(buffer, 0, buffer.Length)) != 0)
        {
            destination.Write(buffer, 0, read);
        }
    }

    public void CopyTo(Stream destination)
    {
        CopyTo(destination, 81920); // Default buffer size used by Stream.CopyTo
    }

    public Task CopyToAsync(Stream destination)
    {
        return CopyToAsync(destination, 81920);
    }

    public Task CopyToAsync(Stream destination, int bufferSize)
    {
        return CopyToAsync(destination, bufferSize, CancellationToken.None);
    }

    public Task CopyToAsync(Stream destination, CancellationToken cancellationToken)
    {
        return CopyToAsync(destination, 81920, cancellationToken);
    }

    public async Task CopyToAsync(Stream destination, int bufferSize, CancellationToken cancellationToken)
    {
        if (destination == null)
            throw new ArgumentNullException(nameof(destination));
        if (bufferSize <= 0)
            throw new ArgumentOutOfRangeException(nameof(bufferSize));

        var buffer = new byte[bufferSize];
        int read;
        while ((read = await ReadAsync(buffer, 0, buffer.Length, cancellationToken)) != 0)
        {
            await destination.WriteAsync(buffer.AsMemory(0, read), cancellationToken);
        }
    }


    public async ValueTask DisposeAsync()
    {
        if (!isDisposed)
        {
            isDisposed = true;
            segments.Clear();
            GC.SuppressFinalize(this);
        }
        await ValueTask.CompletedTask;
    }

    public int EndRead(IAsyncResult asyncResult)
    {
        if (asyncResult is Task<int> task)
            return task.GetAwaiter().GetResult();
        throw new ArgumentException("Invalid IAsyncResult", nameof(asyncResult));
    }

    public void EndWrite(IAsyncResult asyncResult)
    {
        if (asyncResult is Task task)
            task.GetAwaiter().GetResult();
        else
            throw new ArgumentException("Invalid IAsyncResult", nameof(asyncResult));
    }

    public void Flush(bool flushToDisk)
    {
        // No-op as writes are already handled atomically
    }

    public Task FlushAsync()
    {
        return Task.CompletedTask;
    }

    public Task FlushAsync(CancellationToken cancellationToken)
    {
        return Task.CompletedTask;
    }

    public int Read(Span<byte> buffer)
    {
        if (!CanRead)
            throw new NotSupportedException("Stream does not support reading");

        if (isDisposed)
            throw new ObjectDisposedException(nameof(EmailDBFileStream));

        if (!segments.Any() || position >= segments[0].ContentLength)
            return 0;

        var segment = segments[0];
        var available = segment.ContentLength - position;
        var toRead = Math.Min(buffer.Length, available);

        segment.SegmentData.AsSpan((int)position, (int)toRead).CopyTo(buffer);
        position += toRead;
        return (int)toRead;
    }

    public int ReadFaster(byte[] buffer, int offset, int count)
    {
        return Read(buffer, offset, count); // Use standard Read implementation
    }

    public ValueTask<int> ReadAsync(Memory<byte> buffer, CancellationToken cancellationToken = default)
    {
        try
        {
            return new ValueTask<int>(Read(buffer.Span));
        }
        catch (Exception ex)
        {
            return new ValueTask<int>(Task.FromException<int>(ex));
        }
    }

    public Task<int> ReadAsync(byte[] buffer, int offset, int count, CancellationToken cancellationToken)
    {
        try
        {
            return Task.FromResult(Read(buffer, offset, count));
        }
        catch (Exception ex)
        {
            return Task.FromException<int>(ex);
        }
    }

    public Task<int> ReadAsync(byte[] buffer, int offset, int count)
    {
        return ReadAsync(buffer, offset, count, CancellationToken.None);
    }

    public int ReadByte()
    {
        var buffer = new byte[1];
        return Read(buffer, 0, 1) == 1 ? buffer[0] : -1;
    }
    public void Write(ReadOnlySpan<byte> buffer)
    {
        if (!CanWrite)
            throw new NotSupportedException("Stream does not support writing");

        if (isDisposed)
            throw new ObjectDisposedException(nameof(EmailDBFileStream));

        var array = buffer.ToArray();
        Write(array, 0, array.Length);
    }

    public Task WriteAsync(byte[] buffer, int offset, int count)
    {
        return WriteAsync(buffer, offset, count, CancellationToken.None);
    }

    public Task WriteAsync(byte[] buffer, int offset, int count, CancellationToken cancellationToken)
    {
        try
        {
            Write(buffer, offset, count);
            return Task.CompletedTask;
        }
        catch (Exception ex)
        {
            return Task.FromException(ex);
        }
    }

    public ValueTask WriteAsync(ReadOnlyMemory<byte> buffer, CancellationToken cancellationToken = default)
    {
        try
        {
            Write(buffer.Span);
            return ValueTask.CompletedTask;
        }
        catch (Exception ex)
        {
            return new ValueTask(Task.FromException(ex));
        }
    }

    public void WriteByte(byte value)
    {
        Write(new[] { value }, 0, 1);
    }

    public Stream ToStream()
    {
        throw new NotImplementedException();
    }
}
\RandomAccessDevice.cs
using EmailDB.Format.Models;
using Tenray.ZoneTree.Segments.Block;
using Tenray.ZoneTree.Segments.RandomAccess;

namespace EmailDB.Format.ZoneTree;

public sealed class RandomAccessDevice : IRandomAccessDevice
{
    private readonly SegmentManager segmentManager;
    private readonly long segmentId;
    private readonly string category;
    private long currentPosition;
    private int deviceLength;
    private volatile bool isDisposed;
    private volatile bool isSealed;
    private SegmentContent currentSegment;

    public bool Writable { get; private set; }
    public long Length => deviceLength;
    public int ReadBufferCount => 0;
    public string FilePath { get; }
    public long SegmentId => segmentId;

    public RandomAccessDevice(
        SegmentManager segmentManager,       
        long segmentId,
        string category,
        bool writable)
    {
        this.segmentManager = segmentManager;
        
        this.segmentId = segmentId;
        this.category = category;
        Writable = writable;
        FilePath = $"{segmentId}_{category}";

        if (!writable)
        {
            // For read-only device, load existing segment content
            if(segmentManager.TryGetSegment(FilePath, out  SegmentContent segment))
            {
                currentSegment = segment;
                deviceLength = segment.ContentLength;            }
            
            if (segment != null)
            {
                currentSegment = segment;
                deviceLength = segment.ContentLength;
            }
        }
        else
        {
            // For writable device, create new segment
            currentSegment = new SegmentContent
            {
                SegmentId = segmentId,
                SegmentData = new byte[0],
                ContentLength = 0,
                SegmentTimestamp = DateTimeOffset.UtcNow.ToUnixTimeSeconds(),
                Version = 1
            };
        }
    }

    public long AppendBytesReturnPosition(Memory<byte> bytes)
    {
        if (!Writable || isSealed || isDisposed)
            throw new InvalidOperationException("Device is not writable, sealed or disposed");

        var position = currentPosition;

        // Create new byte array with appended data
        var newData = new byte[currentSegment.SegmentData.Length + bytes.Length];
        if (currentSegment.SegmentData.Length > 0)
            currentSegment.SegmentData.CopyTo(newData, 0);
        bytes.CopyTo(newData.AsMemory(currentSegment.SegmentData.Length));

        // Update segment
        currentSegment.SegmentData = newData;
        currentSegment.ContentLength += bytes.Length;
        deviceLength = currentSegment.ContentLength;
        currentPosition += bytes.Length;

        return position;
    }

    public Memory<byte> GetBytes(long offset, int length, SingleBlockPin blockPin = null)
    {
        if (offset >= deviceLength)
            return Memory<byte>.Empty;

        var available = deviceLength - offset;
        var bytesToRead = Math.Min(length, available);

        return new Memory<byte>(currentSegment.SegmentData, (int)offset, (int)bytesToRead);
    }

    public void ClearContent()
    {
        if (!Writable || isSealed || isDisposed)
            throw new InvalidOperationException("Device is not writable, sealed or disposed");

        currentSegment.SegmentData = new byte[0];
        currentSegment.ContentLength = 0;
        deviceLength = 0;
        currentPosition = 0;
    }

    public void Close()
    {
        if (!isDisposed)
        {
            if (Writable && !isSealed)
            {
                // Write final segment data before closing
                WriteSegment();
            }
            isDisposed = true;
        }
    }

    private void WriteSegment()
    {
        segmentManager.WriteSegment(currentSegment);     
    }

    public void Delete()
    {
        if (Writable)
        {
            segmentManager.DeleteSegment(FilePath);
        }
        isDisposed = true;
    }

    public void Dispose()
    {
        Close();
    }

    public void SealDevice()
    {
        if (Writable && !isSealed)
        {
            WriteSegment();
            isSealed = true;
            Writable = false;
        }
    }

    public int ReleaseInactiveCachedBuffers(long ticks)
    {
        // No caching implemented
        return 0;
    }
}
\RandomAccessDeviceManager.cs
using Tenray.ZoneTree.AbstractFileStream;
using Tenray.ZoneTree.Core;
using Tenray.ZoneTree.Options;
using Tenray.ZoneTree.Segments.RandomAccess;

namespace EmailDB.Format.ZoneTree;

public class RandomAccessDeviceManager : IRandomAccessDeviceManager
{
    private readonly SegmentManager storageManager;
    private readonly string name;
    private readonly Dictionary<string, IRandomAccessDevice> devices;

    public RandomAccessDeviceManager(SegmentManager storageManager, string name)
    {
        this.storageManager = storageManager;
        this.name = name;
        this.devices = new Dictionary<string, IRandomAccessDevice>();
    }

    public int DeviceCount => devices.Count;
    public int ReadOnlyDeviceCount => devices.Values.Count(d => !d.Writable);
    public int WritableDeviceCount => devices.Values.Count(d => d.Writable);
    public IFileStreamProvider FileStreamProvider => new EmailDBFileStreamProvider(storageManager);

    public IRandomAccessDevice CreateWritableDevice(
        long segmentId,
        string category,
        bool isCompressed,
        int compressionBlockSize,
        bool deleteIfExists,
        bool backupIfDelete,
        CompressionMethod compressionMethod,
        int compressionLevel)
    {
        var key = GetDeviceKey(segmentId, category, isCompressed);

        if (devices.TryGetValue(key, out var existing))
        {
            if (deleteIfExists)
            {
                if (backupIfDelete)
                {
                    // TODO: Implement backup
                }
                existing.Delete();
                devices.Remove(key);
            }
            else
            {
                return existing;
            }
        }

        var device = new RandomAccessDevice(storageManager, segmentId, category, true);
        devices[key] = device;
        return device;
    }

    public IRandomAccessDevice GetReadOnlyDevice(
        long segmentId,
        string category,
        bool isCompressed,
        int compressionBlockSize,
        CompressionMethod compressionMethod,
        int compressionLevel)
    {
        var key = GetDeviceKey(segmentId, category, isCompressed);

        if (devices.TryGetValue(key, out var existing))
        {
            return existing;
        }

        var device = new RandomAccessDevice(storageManager, segmentId, category, false);
        devices[key] = device;
        return device;
    }

    public void DeleteDevice(long segmentId, string category, bool isCompressed)
    {
        var key = GetDeviceKey(segmentId, category, isCompressed);
        if (devices.TryGetValue(key, out var device))
        {
            device.Delete();
            devices.Remove(key);
        }
    }

    public bool DeviceExists(long segmentId, string category, bool isCompressed)
    {
        var key = GetDeviceKey(segmentId, category, isCompressed);
        return devices.ContainsKey(key);
    }

    public IReadOnlyList<IRandomAccessDevice> GetDevices()
    {
        return devices.Values.ToList();
    }

    public IReadOnlyList<IRandomAccessDevice> GetReadOnlyDevices()
    {
        return devices.Values.Where(d => !d.Writable).ToList();
    }

    public IReadOnlyList<IRandomAccessDevice> GetWritableDevices()
    {
        return devices.Values.Where(d => d.Writable).ToList();
    }

    public void RemoveReadOnlyDevice(long segmentId, string category)
    {
        var key = GetDeviceKey(segmentId, category, false);
        if (devices.TryGetValue(key, out var device) && !device.Writable)
        {
            device.Close();
            devices.Remove(key);
        }
    }

    public void RemoveWritableDevice(long segmentId, string category)
    {
        var key = GetDeviceKey(segmentId, category, false);
        if (devices.TryGetValue(key, out var device) && device.Writable)
        {
            device.Close();
            devices.Remove(key);
        }
    }

    public void CloseAllDevices()
    {
        foreach (var device in devices.Values)
        {
            device.Close();
        }
        devices.Clear();
    }

    public void DropStore()
    {
        foreach (var device in devices.Values)
        {
            device.Delete();
        }
        devices.Clear();
    }

    public string GetFilePath(long segmentId, string category)
    {
        return $"{name}_{category}_{segmentId}";
    }

    private string GetDeviceKey(long segmentId, string category, bool isCompressed)
    {
        return $"{segmentId}_{category}_{isCompressed}";
    }
}
\WriteAheadLog.cs
using EmailDB.Format.Models;
using ProtoBuf;
using System.Collections.Concurrent;
using Tenray.ZoneTree.Exceptions.WAL;
using Tenray.ZoneTree.WAL;

namespace EmailDB.Format.ZoneTree;

/// <summary>
/// WriteAheadLog implementation using EmailDB storage
/// </summary>
public class WriteAheadLog<TKey, TValue> : IWriteAheadLog<TKey, TValue> , IWriteAheadLogBase
{
    private readonly StorageManager storageManager;
    private readonly string folderName;
    private bool isFrozen;
    private readonly object writeLock = new object();
    private readonly ConcurrentDictionary<long, WALEntry<TKey, TValue>> entryCache;

    public WriteAheadLog(StorageManager storageManager, string name, long segmentId, string category)
    {
        this.storageManager = storageManager;
        this.folderName = $"{name}_wal_{category}_{segmentId}";
        this.entryCache = new ConcurrentDictionary<long, WALEntry<TKey, TValue>>();

        // Ensure WAL folder exists
        storageManager.CreateFolder(folderName);
    }

    public string FilePath => folderName;
    public bool EnableIncrementalBackup { get; set; }
    public int InitialLength { get; private set; }

    public void Append(in TKey key, in TValue value, long opIndex)
    {
        if (isFrozen) return;

        lock (writeLock)
        {
            var entry = new WALEntry<TKey, TValue>
            {
                Key = key,
                Value = value,
                OpIndex = opIndex
            };

            using var ms = new MemoryStream();
            Serializer.SerializeWithLengthPrefix(ms, entry, PrefixStyle.Base128);
            storageManager.AddEmailToFolder(folderName, ms.ToArray());
            entryCache[opIndex] = entry;
        }
    }

    public void Drop()
    {
        lock (writeLock)
        {
            storageManager.DeleteFolder(folderName, true);
            entryCache.Clear();
        }
    }

    public WriteAheadLogReadLogEntriesResult<TKey, TValue> ReadLogEntries(
        bool stopReadOnException,
        bool stopReadOnChecksumFailure,
        bool sortByOpIndexes)
    {
        var entries = new List<WALEntry<TKey, TValue>>();
        var result = new WriteAheadLogReadLogEntriesResult<TKey, TValue>();

        try
        {
            // Read entries from storage
            foreach (var (_, block) in storageManager.WalkBlocks())
            {
                if (block.Content is SegmentContent segment)
                {
                    try
                    {
                        using var ms = new MemoryStream(segment.SegmentData);
                        var entry = Serializer.DeserializeWithLengthPrefix<WALEntry<TKey, TValue>>(ms, PrefixStyle.Base128);
                        if (entry != null)
                        {
                            entries.Add(entry);
                            entryCache[entry.OpIndex] = entry;
                        }
                    }
                    catch (Exception ex)
                    {
                        if (stopReadOnException)
                        {
                            result.Exceptions[entries.Count] = ex;
                            break;
                        }
                    }
                }
            }

            if (sortByOpIndexes)
            {
                entries.Sort((a, b) => a.OpIndex.CompareTo(b.OpIndex));
            }

            result.Success = true;
            result.Keys = entries.Select(e => e.Key).ToList();
            result.Values = entries.Select(e => e.Value).ToList();
            result.MaximumOpIndex = entries.Count > 0 ? entries.Max(e => e.OpIndex) : 0;
            InitialLength = entries.Count;
        }
        catch (Exception ex)
        {
            result.Success = false;
            result.Exceptions[0] = ex;
        }

        return result;
    }

    public long ReplaceWriteAheadLog(TKey[] keys, TValue[] values, bool disableBackup)
    {
        lock (writeLock)
        {
            if (!disableBackup && EnableIncrementalBackup)
            {
                BackupCurrentLog();
            }

            // Clear existing entries
            storageManager.DeleteFolder(folderName, true);
            storageManager.CreateFolder(folderName);
            entryCache.Clear();

            // Write new entries
            for (int i = 0; i < keys.Length; i++)
            {
                var entry = new WALEntry<TKey, TValue>
                {
                    Key = keys[i],
                    Value = values[i],
                    OpIndex = i
                };

                using var ms = new MemoryStream();
                Serializer.SerializeWithLengthPrefix(ms, entry, PrefixStyle.Base128);
                storageManager.AddEmailToFolder(folderName, ms.ToArray());
                entryCache[i] = entry;
            }

            return 0;
        }
    }

    private void BackupCurrentLog()
    {
        var backupFolder = $"{folderName}_backup";
        storageManager.CreateFolder(backupFolder);

        foreach (var entry in entryCache.Values)
        {
            using var ms = new MemoryStream();
            Serializer.SerializeWithLengthPrefix(ms, entry, PrefixStyle.Base128);
            storageManager.AddEmailToFolder(backupFolder, ms.ToArray());
        }
    }

    public void MarkFrozen()
    {
        isFrozen = true;
    }

    public void TruncateIncompleteTailRecord(IncompleteTailRecordFoundException incompleteTailException)
    {
        // No-op as we use atomic operations
    }

    public void Dispose()
    {
        entryCache.Clear();
    }
}

\WriteAheadLogProvider.cs
using global::EmailDB.Format.Models;
using ProtoBuf;
using Tenray.ZoneTree.AbstractFileStream;
using Tenray.ZoneTree.Options;
using Tenray.ZoneTree.Serializers;
using Tenray.ZoneTree.WAL;

namespace EmailDB.Format.ZoneTree;

public class WriteAheadLogProvider : IWriteAheadLogProvider
{
    private readonly StorageManager storageManager;
    private readonly string name;
    private readonly Dictionary<string, IWriteAheadLogBase> logs;
    private long walBlockOffset = -1;

    public WriteAheadLogProvider(StorageManager storageManager, string name)
    {
        this.storageManager = storageManager;
        this.name = name;
        this.logs = new Dictionary<string, IWriteAheadLogBase>();

        // Initialize WAL block if needed
        InitializeWAL();
    }

    private void InitializeWAL()
    {
        var metadata = GetMetadataContent();
        if (metadata != null)
        {
            walBlockOffset = metadata.WALOffset;
            if (walBlockOffset == -1)
            {
                // Create initial WAL block
                var walContent = new WALContent();
                var walBlock = new Block
                {
                    Header = new BlockHeader
                    {
                        Type = BlockType.WAL,
                        Timestamp = DateTimeOffset.UtcNow.ToUnixTimeSeconds(),
                        Version = 1
                    },
                    Content = walContent
                };

                walBlockOffset = storageManager.WriteBlock(walBlock);
                metadata.WALOffset = walBlockOffset;
                var tmpMD = storageManager.segmentManager.GetMetadata();
                tmpMD.WALOffset = walBlockOffset;
                storageManager.segmentManager.UpdateMetadata(tmpMD);
            }
        }
    }

    public void InitCategory(string category)
    {
        var walContent = GetWALContent();
        if (!walContent.CategoryOffsets.ContainsKey(category))
        {
            walContent.CategoryOffsets[category] = -1;
            UpdateWALContent(walContent);
        }
    }

    public IWriteAheadLog<TK, TV> GetOrCreateWAL<TK, TV>(
     long segmentId,
     string category,
     WriteAheadLogOptions options,
     ISerializer<TK> keySerializer,
     ISerializer<TV> valueSerializer)
    {
        var key = GetWALKey(segmentId, category);

        if (logs.TryGetValue(key, out var existing))
        {
            if (existing is IWriteAheadLog<TK, TV> typedExisting)
            {
                return typedExisting;
            }
            throw new InvalidOperationException($"Existing WAL for key '{key}' has incompatible types.");
        }

        var wal = new WriteAheadLog<TK, TV>(
            storageManager,
            name,
            segmentId,
            category);

        logs[key] = wal; // Store the generic WAL
        return wal;
    }

    public IWriteAheadLog<TKey, TValue> GetWAL<TKey, TValue>(long segmentId, string category)
    {
        var key = GetWALKey(segmentId, category);
        if (logs.TryGetValue(key, out var wal))
        {
            return (IWriteAheadLog<TKey, TValue>)wal;
        }
        return null;
    }

    public bool RemoveWAL(long segmentId, string category)
    {
        var key = GetWALKey(segmentId, category);
        if (logs.TryGetValue(key, out var wal))
        {
            wal.Drop();
            return logs.Remove(key);
        }
        return false;
    }

    public void DropStore()
    {
        foreach (var wal in logs.Values)
        {
            wal.Drop();
        }
        logs.Clear();
    }

    internal WALContent GetWALContent()
    {
        if (walBlockOffset != -1)
        {
            var block = storageManager.ReadBlock(walBlockOffset);
            if (block?.Content is WALContent walContent)
            {
                return walContent;
            }
        }
        return new WALContent();
    }

    internal void UpdateWALContent(WALContent content)
    {
        var block = new Block
        {
            Header = new BlockHeader
            {
                Type = BlockType.WAL,
                Timestamp = DateTimeOffset.UtcNow.ToUnixTimeSeconds(),
                Version = 1
            },
            Content = content
        };

        storageManager.WriteBlock(block, walBlockOffset);
    }

    private string GetWALKey(long segmentId, string category)
    {
        return $"{segmentId}_{category}";
    }

    private MetadataContent GetMetadataContent()
    {
        var header = storageManager.GetHeader();
        if (header.FirstMetadataOffset != -1)
        {
            var block = storageManager.ReadBlock(header.FirstMetadataOffset);
            return block?.Content as MetadataContent;
        }
        return null;
    }
}

[ProtoContract]
public class WALEntry<TKey, TValue>
{
    [ProtoMember(1)]
    public TKey Key { get; set; }

    [ProtoMember(2)]
    public TValue Value { get; set; }

    [ProtoMember(3)]
    public long OpIndex { get; set; }
}

public class WriteAheadLogWrapper
{
    public object WriteAheadLog { get; }
    public Type KeyType { get; }
    public Type ValueType { get; }

    public WriteAheadLogWrapper(object writeAheadLog, Type keyType, Type valueType)
    {
        WriteAheadLog = writeAheadLog;
        KeyType = keyType;
        ValueType = valueType;
    }
}

public interface IWriteAheadLogBase : IDisposable
{
    string FilePath { get; }
    bool EnableIncrementalBackup { get; set; }
    int InitialLength { get; }
    void Drop();
    void MarkFrozen();
}
\ZoneTreeFactory.cs
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;
using Tenray.ZoneTree.Logger;
using Tenray.ZoneTree.Options;
using Tenray.ZoneTree;
using Microsoft.Extensions.Options;
using Tenray.ZoneTree.Exceptions;
using Tenray.ZoneTree.Transactional;
using Tenray.ZoneTree.Serializers;
using Tenray.ZoneTree.Comparers;
using ZoneTree.FullTextSearch.Model;
using ZoneTree.FullTextSearch.QueryLanguage;

namespace EmailDB.Format.ZoneTree;
/// <summary>
/// Factory for creating and managing ZoneTree instances integrated with EmailDB storage
/// </summary>
public class EmailDBZoneTreeFactory<TKey, TValue> : IDisposable where TKey : ISerializer<TKey>,IRefComparer<TKey>,new() where TValue : ISerializer<TValue>,new()

{
    private readonly StorageManager _storageManager;
    private readonly bool _enableCompression;
    private readonly CompressionMethod _compressionMethod;
    private readonly ILogger _logger;
    private ZoneTreeFactory<TKey, TValue> Factory { get; set; }

    public EmailDBZoneTreeFactory(StorageManager storageManager,
        bool enableCompression = true,
        CompressionMethod compressionMethod = CompressionMethod.LZ4,
        ILogger logger = null)
    {
        _storageManager = storageManager;
        _enableCompression = enableCompression;
        _compressionMethod = compressionMethod;
        _logger = logger ?? new ConsoleLogger();
    }

    public bool CreateZoneTree(string name)
    {
        Factory = new ZoneTreeFactory<TKey, TValue>();

        // Set up WAL configuration
        var walOptions = new WriteAheadLogOptions
        {
            CompressionMethod = _compressionMethod,
            WriteAheadLogMode = _enableCompression ?
                WriteAheadLogMode.SyncCompressed :
                WriteAheadLogMode.Sync
        };

        // Configure options
        Factory.Configure(options =>
        {
            options.Logger = _logger;
            options.WriteAheadLogOptions = walOptions;

            //options.DiskSegmentOptions = new DiskSegmentOptions
            //{
            //    CompressionMethod = _compressionMethod,
            //    // Customize as needed
            //    MaximumRecordCount = 2000000,
            //    MinimumRecordCount = 100,
            //    CompressionBlockSize = 64 * 1024, // 64KB blocks
            //    CompressionLevel = 2 // Fast compression
            //};
            options.KeySerializer = new TKey();
            options.ValueSerializer = new TValue();
            options.Comparer = new TKey();
            // Set WAL provider to use EmailDB storage
            options.WriteAheadLogProvider = new WriteAheadLogProvider(_storageManager, name);
            // Set up device manager for segment storage
            options.RandomAccessDeviceManager = new RandomAccessDeviceManager(_storageManager.segmentManager, name);
        });

        return true;
    }


    public IZoneTree<TKey, TValue> OpenOrCreate()
    {
        return Factory.OpenOrCreate();
    }

    /// <summary>
    /// Assigns key-value pair deletion query delegate.
    /// </summary>
    /// <param name="isDeleted">The key-value pair deleted query delagate.</param>
    /// <returns>ZoneTree Factory</returns>
    public EmailDBZoneTreeFactory<TKey, TValue>
        SetIsDeletedDelegate(IsDeletedDelegate<TKey, TValue> isDeleted)
    {

        Factory.SetIsDeletedDelegate(isDeleted);
        return this;
    }

    /// <summary>
    /// Assigns value deletion marker delegate.
    /// </summary>
    /// <param name="markValueDeleted">The value deletion marker delegate</param>
    /// <returns>ZoneTree Factory</returns>
    public EmailDBZoneTreeFactory<TKey, TValue>
        SetMarkValueDeletedDelegate(MarkValueDeletedDelegate<TValue> markValueDeleted)
    {
        Factory.SetMarkValueDeletedDelegate(markValueDeleted);
        return this;
    }

    /// <summary>
    /// Sets the key serializer.
    /// </summary>
    /// <param name="keySerializer">The key serializer</param>
    /// <returns>ZoneTree Factory</returns>
    public EmailDBZoneTreeFactory<TKey, TValue>
        SetKeySerializer(ISerializer<TKey> keySerializer)
    {
        Factory.SetKeySerializer(keySerializer);
        return this;
    }

    /// <summary>
    /// Sets the key-comparer.
    /// </summary>
    /// <param name="comparer">The key-comparer.</param>
    /// <returns>ZoneTree Factory</returns>
    public EmailDBZoneTreeFactory<TKey, TValue> SetComparer(IRefComparer<TKey> comparer)
    {
        Factory.SetComparer(comparer);
        return this;
    }

    public void Dispose()
    {
        return;
    }
}
\EmailDBTestSuite.cs
using System.Collections.Concurrent;
using System.Diagnostics;
using System.Text;
using EmailDB.Format;
using EmailDB.Format.Models;
using EmailDB.Testing.Tests;
using MimeKit;

public class EmailDBTestSuite
{
    private const string TestFilePath = "test_email_store.dat";
    private const string CompactedFilePath = "test_email_store_compacted.dat";
    private readonly ITestLogger logger;
    private readonly Dictionary<string, List<TestResult>> testResults = new();
    private readonly Stopwatch stopwatch = new();

    public EmailDBTestSuite(ITestLogger logger = null)
    {
        this.logger = logger ?? new ConsoleTestLogger();
    }

    public async Task RunAllTests()
    {
        CleanupTestFiles();

        await RunTestGroup("Basic Operations", async () =>
        {
            await this.TestBasicFileOperations(TestFilePath);
            await this.TestConcurrentAccess(TestFilePath);
         //   await TestHeaderValidation();
        });

        //await RunTestGroup("Email Management", async () =>
        //{
        //    await TestEmailAddition();
        //    await TestEmailRetrieval();
        //    await TestEmailDeletion();
        //    await TestEmailMovement();
        //    await TestEmailSearch();
        //});

        //await RunTestGroup("Folder Management", async () =>
        //{
        //    await TestFolderCreation();
        //    await TestFolderHierarchy();
        //    await TestFolderRename();
        //    await TestFolderDeletion();
        //    await TestFolderLocking();
        //});

        //await RunTestGroup("Cache Management", async () =>
        //{
        //    await TestCacheHits();
        //    await TestCacheMisses();
        //    await TestCacheEviction();
        //    await TestCacheInvalidation();
        //});

        //await RunTestGroup("Data Integrity", async () =>
        //{
        //    await TestChecksum();
        //    await TestCorruptionRecovery();
        //    await TestJournaling();
        //    await TestTransactionRollback();
        //});

        //await RunTestGroup("Performance", async () =>
        //{
        //    await TestLargeFileHandling();
        //    await TestBulkOperations();
        //    await TestSearchPerformance();
        //});

        //await RunTestGroup("Error Handling", async () =>
        //{
        //    await TestInvalidOperations();
        //    await TestResourceExhaustion();
        //    await TestConcurrencyConflicts();
        //});

        ReportResults();
    }

   

    public void AssertTrue(bool condition, string message)
    {
        if (!condition)
            throw new TestException(message);
    }

    public void AssertNotNull(object obj, string message)
    {
        if (obj == null)
            throw new TestException(message);
    }

    public void AssertEquals<T>(T expected, T actual, string message)
    {
        if (!EqualityComparer<T>.Default.Equals(expected, actual))
            throw new TestException($"{message}. Expected: {expected}, Actual: {actual}");
    }

    public async Task RunTestGroup(string groupName, Func<Task> testFunc)
    {
        logger.LogGroupStart(groupName);
        testResults[groupName] = new List<TestResult>();

        try
        {
            await testFunc();
        }
        catch (Exception ex)
        {
            logger.LogError($"Test group {groupName} failed: {ex.Message}");
            testResults[groupName].Add(new TestResult(groupName, "Group Execution", false, ex.Message));
        }

        logger.LogGroupEnd(groupName);
    }

    private void ReportResults()
    {
        logger.LogSection("Test Results Summary");

        int totalTests = 0;
        int passedTests = 0;

        foreach (var group in testResults)
        {
            var groupResults = group.Value;
            var groupPassed = groupResults.Count(r => r.Success);

            logger.LogGroupResult(group.Key, groupPassed, groupResults.Count);
            totalTests += groupResults.Count;
            passedTests += groupPassed;
        }

        logger.LogFinalSummary(passedTests, totalTests);
    }

    public void CleanupTestFiles()
    {
        if (File.Exists(TestFilePath))
            File.Delete(TestFilePath);
        if (File.Exists(CompactedFilePath))
            File.Delete(CompactedFilePath);
    }

    public FolderContent GetFolder(BlockManager blockManager, string folderName)
    {
        foreach (var (_, block) in blockManager.WalkBlocks())
        {
            if (block.Content is FolderContent folder && folder.Name == folderName)
                return folder;
        }
        return null;
    }

    public void AddSampleEmail(StorageManager storage, string folderName)
    {
        var message = new MimeMessage();
        message.From.Add(new MailboxAddress("Sender", "sender@test.com"));
        message.To.Add(new MailboxAddress("Recipient", "recipient@test.com"));
        message.Subject = "Test Email";
        message.Body = new TextPart("plain") { Text = "This is a test email." };

        using var ms = new MemoryStream();
        message.WriteTo(ms);
        storage.AddEmailToFolder(folderName, ms.ToArray());
    }
}

public interface ITestLogger
{
    void LogGroupStart(string groupName);
    void LogGroupEnd(string groupName);
    void LogTestResult(string testName, bool success, string message = null);
    void LogError(string message);
    void LogSection(string sectionName);
    void LogGroupResult(string groupName, int passed, int total);
    void LogFinalSummary(int totalPassed, int totalTests);
}

public class ConsoleTestLogger : ITestLogger
{
    public void LogGroupStart(string groupName) =>
        Console.WriteLine($"\n=== Starting Test Group: {groupName} ===");

    public void LogGroupEnd(string groupName) =>
        Console.WriteLine($"=== Completed Test Group: {groupName} ===\n");

    public void LogTestResult(string testName, bool success, string message = null)
    {
        var status = success ? "PASSED" : "FAILED";
        Console.WriteLine($"{testName}: {status}");
        if (!success && message != null)
            Console.WriteLine($"  Error: {message}");
    }

    public void LogError(string message) =>
        Console.WriteLine($"ERROR: {message}");

    public void LogSection(string sectionName) =>
        Console.WriteLine($"\n=== {sectionName} ===");

    public void LogGroupResult(string groupName, int passed, int total) =>
        Console.WriteLine($"{groupName}: {passed}/{total} tests passed");

    public void LogFinalSummary(int totalPassed, int totalTests) =>
        Console.WriteLine($"\nFinal Results: {totalPassed}/{totalTests} tests passed " +
                         $"({(totalPassed * 100.0 / totalTests):F1}% success rate)");
}

public class TestResult
{
    public string GroupName { get; }
    public string TestName { get; }
    public bool Success { get; }
    public string Message { get; }

    public TestResult(string groupName, string testName, bool success, string message = null)
    {
        GroupName = groupName;
        TestName = testName;
        Success = success;
        Message = message;
    }
}

public class TestException : Exception
{
    public TestException(string message) : base(message) { }
}

\Program.cs
using System.Diagnostics;
using System.Text;
using EmailDB.Format;
using EmailDB.Format.Models;

class Program
{
    const string filePath = "test_email_store.dat";
    const string compactedFilePath = "test_email_store_compacted.dat";

    static void Main()
    {
        // Clean up any existing test files
        CleanupTestFiles();

        // Run core functionality tests
        TestBasicOperations();
        TestEmailOperations();
        TestFolderOperations();

        // Run performance tests
        TestPerformance();

        // Run robustness tests
        TestRobustness();

        // Test maintenance operations
        TestCompaction();

        // Test error handling
        TestErrorHandling();
    }

    static void CleanupTestFiles()
    {
        if (File.Exists(filePath))
            File.Delete(filePath);
        if (File.Exists(compactedFilePath))
            File.Delete(compactedFilePath);
    }

    static void TestBasicOperations()
    {
        Console.WriteLine("\n=== Testing Basic Operations ===");

        using var storage = new StorageManager(filePath, createNew: true);

        // Create basic folder structure
        Console.WriteLine("Creating initial folder structure...");
        storage.CreateFolder("Root");
        storage.CreateFolder("Inbox", "Root");
        storage.CreateFolder("Sent", "Root");
        storage.CreateFolder("Archive", "Root");

        // Add some initial emails
        Console.WriteLine("Adding initial emails...");
        AddSampleEmails(storage, "Inbox", 3);
        AddSampleEmails(storage, "Sent", 2);

        DumpFileStructure(storage.blockManager);
    }

    static void TestEmailOperations()
    {
        Console.WriteLine("\n=== Testing Email Operations ===");

        using var storage = new StorageManager(filePath);

        // Add new emails and measure performance
        Console.WriteLine("Adding new emails to multiple folders...");
        var stopwatch = Stopwatch.StartNew();

        AddSampleEmails(storage, "Inbox", 5);
        AddSampleEmails(storage, "Sent", 3);

        stopwatch.Stop();
        Console.WriteLine($"Time taken to add emails: {stopwatch.ElapsedMilliseconds}ms");

        // Test email operations with cache
        Console.WriteLine("\nTesting cached folder access...");
        stopwatch.Restart();

        // First access (uncached)
        var folder = GetFolder(storage.blockManager, "Inbox");
        var firstAccessTime = stopwatch.ElapsedMilliseconds;

        // Second access (should be cached)
        stopwatch.Restart();
        folder = GetFolder(storage.blockManager, "Inbox");
        var secondAccessTime = stopwatch.ElapsedMilliseconds;

        Console.WriteLine($"First folder access: {firstAccessTime}ms");
        Console.WriteLine($"Second folder access (cached): {secondAccessTime}ms");

        // Test email movement
        Console.WriteLine("\nTesting email movement between folders...");
        if (folder?.EmailIds.Count > 0)
        {
            var emailToMove = folder.EmailIds[0];
            storage.MoveEmail(emailToMove, "Inbox", "Archive");
            Console.WriteLine($"Moved email {emailToMove} from Inbox to Archive");
        }

        DumpFileStructure(storage.blockManager);
    }

    static void TestPerformance()
    {
        Console.WriteLine("\n=== Testing Performance ===");

        using var storage = new StorageManager(filePath);
        var stopwatch = Stopwatch.StartNew();

        // Test folder tree access performance
        Console.WriteLine("Testing folder tree access times...");
        for (int i = 0; i < 5; i++)
        {
            stopwatch.Restart();
            var tree = GetLatestFolderTree(storage.blockManager);
            Console.WriteLine($"Folder tree access #{i + 1}: {stopwatch.ElapsedMilliseconds}ms");
        }

        // Test folder access with cache
        Console.WriteLine("\nTesting folder access with cache...");
        string[] folders = { "Inbox", "Sent", "Archive" };

        foreach (var folderName in folders)
        {
            stopwatch.Restart();
            var folder = GetFolder(storage.blockManager, folderName);
            var firstAccess = stopwatch.ElapsedMilliseconds;

            stopwatch.Restart();
            folder = GetFolder(storage.blockManager, folderName);
            var secondAccess = stopwatch.ElapsedMilliseconds;

            Console.WriteLine($"Folder '{folderName}':");
            Console.WriteLine($"  First access: {firstAccess}ms");
            Console.WriteLine($"  Second access (cached): {secondAccess}ms");
        }

        // Test cache invalidation
        Console.WriteLine("\nTesting cache invalidation...");
        storage.InvalidateCache();

        stopwatch.Restart();
        var folderAfterInvalidation = GetFolder(storage.blockManager, "Inbox");
        Console.WriteLine($"Folder access after cache invalidation: {stopwatch.ElapsedMilliseconds}ms");
    }

    static void TestRobustness()
    {
        Console.WriteLine("\n=== Testing Robustness ===");

        using var storage = new StorageManager(filePath);

        // Test recovery from invalid offsets
        Console.WriteLine("Testing recovery from invalid metadata...");
        try
        {
            // Force cache invalidation to test recovery
            storage.InvalidateCache();
            var folder = GetFolder(storage.blockManager, "Inbox");
            Console.WriteLine("Successfully recovered folder information after cache invalidation");
        }
        catch (Exception ex)
        {
            Console.WriteLine($"Error during recovery: {ex.Message}");
        }

        // Test concurrent operations
        Console.WriteLine("\nTesting concurrent operations...");
        var tasks = new List<Task>();

        for (int i = 0; i < 5; i++)
        {
            tasks.Add(Task.Run(() => {
                try
                {
                    AddSampleEmails(storage, "Inbox", 1);
                }
                catch (Exception ex)
                {
                    Console.WriteLine($"Concurrent operation error: {ex.Message}");
                }
            }));
        }

        Task.WaitAll(tasks.ToArray());
        Console.WriteLine("Completed concurrent operation testing");
    }

    static void TestFolderOperations()
    {
        Console.WriteLine("\n=== Testing Folder Operations ===");

        using var storage = new StorageManager(filePath);
        var stopwatch = Stopwatch.StartNew();

        // Create new folders
        Console.WriteLine("Creating new folders...");
        storage.CreateFolder("Important", "Root");
        storage.CreateFolder("Temporary", "Root");

        // Test folder access performance
        Console.WriteLine("\nTesting folder access performance...");
        stopwatch.Restart();
        var folder1 = GetFolder(storage.blockManager, "Important");
        var firstAccess = stopwatch.ElapsedMilliseconds;

        stopwatch.Restart();
        var folder2 = GetFolder(storage.blockManager, "Important");
        var cachedAccess = stopwatch.ElapsedMilliseconds;

        Console.WriteLine($"First folder access: {firstAccess}ms");
        Console.WriteLine($"Cached folder access: {cachedAccess}ms");

        // Add emails and test folder updates
        AddSampleEmails(storage, "Important", 2);
        AddSampleEmails(storage, "Temporary", 3);

        // Test folder deletion with cleanup
        Console.WriteLine("\nTesting folder deletion with cleanup...");
        storage.DeleteFolder("Temporary", deleteEmails: true);

        DumpFileStructure(storage.blockManager);
    }

    static void TestCompaction()
    {
        Console.WriteLine("\n=== Testing Compaction ===");

        using var storage = new StorageManager(filePath);

        // Create test data for compaction
        Console.WriteLine("Creating test data for compaction...");
        for (int i = 0; i < 3; i++)
        {
            AddSampleEmails(storage, "Inbox", 2);
            // Update some emails to create outdated content
            var folder = GetFolder(storage.blockManager, "Inbox");
            if (folder?.EmailIds.Count > 0)
            {
                var emailId = folder.EmailIds[0];
                var newContent = Encoding.UTF8.GetBytes($"Updated content {i} " + Guid.NewGuid());
                storage.UpdateEmailContent(emailId, newContent);
            }
        }

        Console.WriteLine("\nFile structure before compaction:");
        DumpFileStructure(storage.blockManager);

        // Perform compaction
        Console.WriteLine("\nPerforming compaction...");
        var stopwatch = Stopwatch.StartNew();
        storage.Compact(compactedFilePath);
        Console.WriteLine($"Compaction completed in {stopwatch.ElapsedMilliseconds}ms");

        // Compare files
        var originalSize = new FileInfo(filePath).Length;
        var compactedSize = new FileInfo(compactedFilePath).Length;
        var reduction = (1 - (double)compactedSize / originalSize) * 100;

        Console.WriteLine($"\nCompaction Results:");
        Console.WriteLine($"Original size: {originalSize / 1024.0:F2} KB");
        Console.WriteLine($"Compacted size: {compactedSize / 1024.0:F2} KB");
        Console.WriteLine($"Size reduction: {reduction:F1}%");

        // Verify compacted file
        Console.WriteLine("\nVerifying compacted file...");
        using (var compactedStorage = new StorageManager(compactedFilePath))
        {
            DumpFileStructure(compactedStorage.blockManager);
        }
    }

    static void TestErrorHandling()
    {
        Console.WriteLine("\n=== Testing Error Handling ===");

        using var storage = new StorageManager(filePath);

        // Test invalid operations
        Console.WriteLine("Testing invalid operations...");

        // Test invalid folder creation
        try
        {
            storage.CreateFolder("Root");
            Console.WriteLine("ERROR: Should not allow duplicate root folder");
        }
        catch (Exception ex)
        {
            Console.WriteLine($"Expected error: {ex.Message}");
        }

        // Test nonexistent folder operations
        try
        {
            storage.DeleteFolder("NonexistentFolder");
            Console.WriteLine("ERROR: Should not allow deleting nonexistent folder");
        }
        catch (Exception ex)
        {
            Console.WriteLine($"Expected error: {ex.Message}");
        }

        // Test invalid email operations
        try
        {            
            storage.MoveEmail(new EmailHashedID(), "Inbox", "Archive");
            Console.WriteLine("ERROR: Should not allow moving nonexistent email");
        }
        catch (Exception ex)
        {
            Console.WriteLine($"Expected error: {ex.Message}");
        }

        // Test cache behavior with invalid data
        Console.WriteLine("\nTesting cache behavior with invalid data...");
        storage.InvalidateCache();
        try
        {
            var folder = GetFolder(storage.blockManager, "Inbox");
            Console.WriteLine("Successfully recovered from cache invalidation");
        }
        catch (Exception ex)
        {
            Console.WriteLine($"Error during cache recovery: {ex.Message}");
        }
    }

    // Helper methods
    static void AddSampleEmails(StorageManager storage, string folderName, int count)
    {
        var random = new Random();
        for (int i = 0; i < count; i++)
        {
            var contentSize = random.Next(5 * 1024, 20 * 1024);
            var content = new byte[contentSize];
            random.NextBytes(content);

            try
            {
                storage.AddEmailToFolder(folderName, content);
            }
            catch (Exception ex)
            {
                Console.WriteLine($"Error adding email to {folderName}: {ex.Message}");
            }
        }
    }

    static FolderContent GetFolder(BlockManager storage, string folderName)
    {
        foreach (var (_, block) in storage.WalkBlocks())
        {
            if (block.Content is FolderContent folder && folder.Name == folderName)
            {
                return folder;
            }
        }
        return null;
    }

    static FolderTreeContent GetLatestFolderTree(BlockManager storage)
    {
        FolderTreeContent latest = null;
        foreach (var (_, block) in storage.WalkBlocks())
        {
            if (block.Content is FolderTreeContent tree)
            {
                latest = tree;
            }
        }
        return latest;
    }

    static void DumpFileStructure(BlockManager storage)
    {
        Console.WriteLine("\n=== File Structure Summary ===");
        var stats = new Dictionary<BlockType, int>();
        var emailsByFolder = new Dictionary<string, int>();
        var totalEmails = 0;
        var totalEmailSize = 0L;
        var uniqueSegments = new HashSet<long>();

        foreach (var (_, block) in storage.WalkBlocks())
        {
            if (!stats.ContainsKey(block.Header.Type))
                stats[block.Header.Type] = 0;
            stats[block.Header.Type]++;

            switch (block.Content)
            {
                case FolderContent folder:
                    emailsByFolder[folder.Name] = folder.EmailIds.Count;
                    totalEmails += folder.EmailIds.Count;
                    break;
                case SegmentContent segment:
                    uniqueSegments.Add(segment.SegmentId);
                    totalEmailSize += segment.SegmentData.Length;
                    break;
            }
        }

        Console.WriteLine("Block counts by type:");
        foreach (var stat in stats.OrderBy(x => x.Key))
        {
            Console.WriteLine($"  {stat.Key}: {stat.Value}");
        }

        Console.WriteLine("\nEmails per folder:");
        foreach (var folder in emailsByFolder.OrderBy(x => x.Key))
        {
            Console.WriteLine($"  {folder.Key}: {folder.Value} emails");
        }

        Console.WriteLine($"\nTotal statistics:");
        Console.WriteLine($"  Total emails: {totalEmails:N0}");
        Console.WriteLine($"  Unique segments: {uniqueSegments.Count:N0}");
        Console.WriteLine($"  Total email content size: {totalEmailSize / 1024.0:F2} KB");
        Console.WriteLine($"  File size: {new FileInfo(filePath).Length / 1024.0:F2} KB");
    }
}
\BasicFileTests.cs
using EmailDB.Format;
using EmailDB.Format.Models;
using System;
using System.Collections.Concurrent;
using System.Collections.Generic;
using System.IO;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace EmailDB.Testing.Tests;

public static class  BasicFileTests
{

    public static async Task TestBasicFileOperations(this EmailDBTestSuite suite, string TestFilePath)
    {      
        using StorageManager storage = new StorageManager(TestFilePath, createNew: true);
        storage.InitializeNewFile();
        storage.Dispose();

        using FileStream fileStream = new FileStream(TestFilePath, FileMode.OpenOrCreate, FileAccess.ReadWrite, FileShare.None);

        // Initialize components
        var blockManager = new BlockManager(fileStream);
        var cacheManager = new CacheManager(blockManager);
        var metadataManager = new MetadataManager(blockManager);
        var folderManager = new FolderManager(blockManager, metadataManager);

        // Test file creation
        suite.AssertTrue(File.Exists(TestFilePath), "Storage file should be created");

        // Test header initialization
        var header = cacheManager.GetHeader();
        suite.AssertNotNull(header, "Header should be initialized");
        suite.AssertEquals(1, header.FileVersion, "File version should be 1");

        // Test basic folder operations
        var folder = folderManager.GetSubfolders("/");

        suite.AssertNotNull(folder, "Root folder should exist");
    }

    public static async Task TestConcurrentAccess(this EmailDBTestSuite suite, string TestFilePath)
    {
        using var storage = new StorageManager(TestFilePath);
        var tasks = new List<Task>();
        var errors = new ConcurrentBag<Exception>();

        // Create multiple concurrent operations
        for (int i = 0; i < 10; i++)
        {
            tasks.Add(Task.Run(() =>
            {
                try
                {
                    storage.CreateFolder($"TestFolder_{i}");
                    suite.AddSampleEmail(storage, $"TestFolder_{i}");
                }
                catch (Exception ex)
                {
                    errors.Add(ex);
                }
            }));
        }

        await Task.WhenAll(tasks);
        suite.AssertEquals(0, errors.Count, "No errors should occur during concurrent access");
    }

   
}

